Version git status:
 - CNAME placement
 - updating the CNAME reference in the conf.py of the docs
 - moving the CNAME to static...
 - removing statsmodels from the docs requirements file as it is causing build issues
 - Merge pull request #14 from jameshtwose/new_docstrings
 - adding docstrings to all the jmspack functions - dealing with pre-commit issues
 - adding the CNAME to deploy the docs directly from github
 - dealing with pytest issues
 - dealing with some pre-commit issues - a lot of the pytests need to be updated
 - adding the imputation_utils notebook to the docs
 - Merge pull request #12 from jameshtwose/imputation_utils
 - making the imputation_utils module and notebook - needs more documentation
 - making the imputation_utils module and notebook - needs more documentation
 - Update README.md
 - Adding the new jmspack logo to the docs
 - starting the missingness playground stuff
 - attempting to use pre-build-command sphinx.yml V4
 - attempting to use pre-build-command sphinx.yml V3
 - attempting to use pre-build-command sphinx.yml V2
 - attempting to use pre-build-command sphinx.yml
 - attempting to sudo apt get install pandoc in the sphinx.yml
 - adding extra notebooks to the documentation to show off the examples
 - updating black version in pre-commit-config
 - adding badges to the readme
 - attempting to add codecov
 - attempting to add codecov
 - updating python-package.yml name
 - removing psycopg2 from the sphinx requirements
 - making psycopg2 an older requirement
 - update the publish yaml to include steady publish release
 - Merge pull request #11 from jameshtwose/develop
 - updating the docstrings in all the functions and creating documentation via sphinx
 - creating tests for the fluctuation_intensity, distribution_uniformity, and complexity_resonance
 - adding the casnet versions of the fi, du, crd, ccp
 - adding the casnet versions of the NLTSA datasets
 - adding the casnet versions of the NLTSA datasets
 - Update python-publish.yml
 - Update python-publish.yml
 - creating fi, du, crd and ccp dfs based on the time_series_dataset.csv in R to benchmark the python versions of those functions
 - Create .pypirc
 - changing the authentication method from user login to API token
 - making the package publish on every push to main
 - Create python-publish.yml
 - Merge pull request #10 from jameshtwose/develop
 - removing the test_ax_attributes test as this opens an interactive window which is not possible in automated releases
 - Update python-package.yml
 - Create python-package.yml
 - creating a flexible summary NLTSA window function in the playground which accepts decomposition models from sklearn
 - Update README.md
 - Merge pull request #6 from jameshtwose/release-0.1.1
 - updating the documentation
 - Merge pull request #5 from jameshtwose/release-0.1.1
 - Bumped version number to 0.1.1

Version 0.1.1:
 - Update build_instructions.md
 - Merge pull request #4 from jameshtwose/develop
 - Adding correct_pvalues() and partial_correlation() functions
 - Looking into different sliding window options to try to elucidate changes in time series e.g. using windowed PCA to identify Early Warning Signals
 - Beginning to write tests for the ml_utils functions
 - Cleaning up the ml_utils and utils modules
 - Cleaning up the ml_utils and utils modules
 - updating some of the machine learning utility functions to be more flexible and creating a notebook showing the use of the currently implemented ml_utils
 - beginning the ml_utils module
 - Update README.md
 - Adding new badges to the README
 - Merge pull request #3 from jameshtwose/release-0.1.0
 - updating the build_instructions
 - Bumping to version 0.1.0

Version 0.1.0
 - adding build instructions and a bump-version shell script to make life a bit easier when creating new package versions
 - Merge pull request #2 from jameshtwose/making_tests
 - creating tests for flatten, apply_scaling and potential_for_change_index functions. Also updating some of the example notebooks
 - writing notebooks for the utils functions, creating some frequentist statistics functions and writing up an optimising NLTSA notebook
 - writing notebooks for the utils functions, creating some frequentist statistics functions and writing up an optimising NLTSA notebook
 - starting to write tests for the utils and internal_utils functions
 - removing superfluous files and folders
 - writing tests and updating notebooks for utils, internal_utils and NLTSA modules
 - writing tests and notebooks for utils, internal_utils, NLTSA
 - adding the utils python file to jmspack - currently this includes a class called jmscolors which provides an easy utility function for getting a set of stylish colours
 - bumping to version 0.0.3
 - Adding create_postgresql_table_based_on_df(), add_data_to_postgresql_table(), delete_postgresql_table() functions to the internal_utils submodule - need to make tests still
 - building the latest version of the documentation
 - updating the setup.py to include the README.md as the long description so it shows as the project description on pypi
 - Merge branch 'main' of github.com:jameshtwose/jmspack into main
 - updating the setup py due to a spelling mistake
 - Update README.md
 - Adding internal_utils to jmspack - currently postgresql functions - data extraction and table names extraction
 - Writing tests for ts_levels function and adding the pre-commit setup, building the latest documentation
 - Merge branch 'main' of github.com:jameshtwose/jmspack into main
 - Adding a time series dataset .csv and adding a notebook showing the use of the NLTSA functions.
 - Update README.md
 - Update README.md
 - Update README.md
 - Update README.md
 - Building the documentation via pdoc3
 - Merge branch 'main' of github.com:jameshtwose/jmspack into main
 - Building the documentation via pdoc3
 - Update README.md
 - Building the documentation via pdoc3
 - Adding the Nonlinear Time Series submodule to jmspack
 - Initiating the jmspack python package
 - Initial commit
