<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>jmspack.frequentist_statistics API documentation</title>
<meta name="description" content="Submodule frequentist_statistics.py includes the following functions: &lt;br&gt;
- **normal_check():** compare the distribution of numeric variables to a …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jmspack.frequentist_statistics</code></h1>
</header>
<section id="section-intro">
<p>Submodule frequentist_statistics.py includes the following functions: <br>
- <strong>normal_check():</strong> compare the distribution of numeric variables to a normal distribution using the
Kolmogrov-Smirnov test <br>
- <strong>correlation_analysis():</strong> Run correlations for numerical features and return output in different formats <br>
- <strong>correlations_as_sample_increases():</strong> Run correlations for subparts of the data to check robustness <br>
- <strong>multiple_univariate_OLSs():</strong> Tmp <br>
- <strong>potential_for_change_index():</strong> Calculate the potential for change index based on either variants of the r-squared
(from linear regression) or the r-value (pearson correlation) <br>
- <strong>correct_pvalues():</strong> function to correct for multiple testing <br>
- <strong>partial_correlation():</strong> function to calculate the partial correlations whilst correcting for other variables <br></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">r&#34;&#34;&#34;Submodule frequentist_statistics.py includes the following functions: &lt;br&gt;
- **normal_check():** compare the distribution of numeric variables to a normal distribution using the
    Kolmogrov-Smirnov test &lt;br&gt;
- **correlation_analysis():** Run correlations for numerical features and return output in different formats &lt;br&gt;
- **correlations_as_sample_increases():** Run correlations for subparts of the data to check robustness &lt;br&gt;
- **multiple_univariate_OLSs():** Tmp &lt;br&gt;
- **potential_for_change_index():** Calculate the potential for change index based on either variants of the r-squared
    (from linear regression) or the r-value (pearson correlation) &lt;br&gt;
- **correct_pvalues():** function to correct for multiple testing &lt;br&gt;
- **partial_correlation():** function to calculate the partial correlations whilst correcting for other variables &lt;br&gt;
&#34;&#34;&#34;
from itertools import combinations
from itertools import product
from typing import Tuple
from typing import Union

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
from matplotlib.lines import Line2D
from scipy import stats
from sklearn.linear_model import LinearRegression
from statsmodels.stats.multitest import multipletests

from .utils import apply_scaling


def normal_check(data: pd.DataFrame) -&gt; pd.DataFrame:
    r&#34;&#34;&#34;Compare the distribution of numeric variables to a normal distribution using the Kolmogrov-Smirnov test
    Wrapper for `scipy.stats.kstest`: the empircal data is compared to a normally distributed variable with the
    same mean and standard deviation. A significant result (p &lt; 0.05) in the goodness of fit test means that the
    data is not normally distributed.
    Parameters
    ----------
    data: pandas.DataFrame
        Dataframe including the columns of interest
    Returns
    ----------
    df_normality_check: pd.DataFrame
        Dataframe with column names, p-values and an indication of normality
    Examples
    ----------
    &gt;&gt;&gt; tips = sns.load_dataset(&#34;tips&#34;)
    &gt;&gt;&gt; df_normality_check = normal_check(tips)
    &#34;&#34;&#34;
    # Select numeric columns only
    num_features = data.select_dtypes(include=&#34;number&#34;).columns.tolist()
    # Compare distribution of each feature to a normal distribution with given mean and std
    df_normality_check = data[num_features].apply(
        lambda x: stats.kstest(
            x.dropna(), stats.norm.cdf, args=(np.nanmean(x), np.nanstd(x)), N=len(x)
        )[1],
        axis=0,
    )

    # create a label that indicates whether a feature has a normal distribution or not
    df_normality_check = pd.DataFrame(df_normality_check).reset_index()
    df_normality_check.columns = [&#34;feature&#34;, &#34;p-value&#34;]
    df_normality_check[&#34;normality&#34;] = df_normality_check[&#34;p-value&#34;] &gt;= 0.05

    return df_normality_check


def permute_test(a, test_type, test, **kwargs):
    r&#34;&#34;&#34;Helper function to run tests for permutations
    Parameters
    ----------
    a : np.array
    test_type: str {&#39;correlation&#39;, &#39;independent_t_test&#39;}
        Type of the test to be used
    test:
        e.g. `scipy.stats.pearsonr` or `statsmodels.stats.weightstats.ttest_ind`
    **kwargs:
        Additional keywords to be added to `test`
        - `a2` for the second feature if test_type = &#39;correlation&#39;
    Returns
    ----------
    float:
        p value for permutation
    &#34;&#34;&#34;
    if test_type == &#34;correlation&#34;:
        a2 = kwargs[&#34;a2&#34;]
        _, p = test(a, a2)

    else:
        raise ValueError(&#34;Unknown test_type provided&#34;)


def correlation_analysis(
    data: pd.DataFrame,
    col_list=None,
    row_list=None,
    check_norm=False,
    method: str = &#34;pearson&#34;,
    dropna: str = &#34;pairwise&#34;,
    permutation_test: bool = False,
    n_permutations: int = 1000,
    random_state=None,
):
    r&#34;&#34;&#34;Run correlations for numerical features and return output in different formats
    Different methods to compute correlations and to handle missing values are implemented.
    Inspired by `researchpy.corr_case` and `researchpy.corr_pair`.
    Parameters
    ----------
    data : pandas.DataFrame
        Dataframe with variables in columns, cases in rows
    row_list: list or None (default: None)
        List with names of columns in `data` that should be in the rows of the correlogram.
        If None, all columns are used but only every unique combination.
    col_list: list or None (default: None)
        List with names of columns in `data` that should be in the columns of the correlogram.
        If None, all columns are used and only every unique combination.
    check_norm: bool (default: False)
        If True, normality will be checked for columns in `data` using `normal_check`. This influences the used method
        for correlations, i.e. Pearson or Spearman. Note: normality check ignores missing values.
    method: {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;}, default &#39;pearson&#39;
        Type of correlation, either Pearson&#39;s r, Spearman&#39;s rho, or Kendall&#39;s tau, implemented via respectively
        `scipy.stats.pearsonr`, `scipy.stats.spearmanr`, and `scipy.stats.kendalltau`
        Will be ignored if check_norm=True. Instead, Person&#39;s r is used for every combination of normally distributed
        columns and Spearman&#39;s rho is used for all other combinations.
    dropna : {&#39;listwise&#39;, &#39;pairwise&#39;}, default &#39;pairwise&#39;
        Should rows with missing values be dropped over the complete `data` (&#39;listwise&#39;) or for every correlation
        separately (&#39;pairwise&#39;)
    permutation_test: bool (default: False)
        If true, a permutation test will added
    n_permutations: int (default: 1000)
        Number of permutations in the permutation test
    random_state: None or int (default: None)
        Random state for permutation_test. If not None, random_state will be updated for every permutation
    plot_permutation: bool (default: False)
        Whether to plot the results of the permutation test
    figsize: tuple (default: (11.7, 8.27))
        Width and height of the figure in inches
    Returns
    ----------
    result_dict: dict
        Dictionary containing with the following keys:
        info : pandas.DataFrame
            Description of correlation method, missing values handling and number of observations
        r-values : pandas.DataFrame
            Dataframe with correlation coefficients. Indices and columns are column names from `data`. Only lower
            triangle is filled.
        p-values : pandas.DataFrame
            Dataframe with p-values. Indices and columns are column names from `data`. Only lower triangle is filled.
        N        : pandas.DataFrame
            Dataframe with numbers of observations. Indices and columns are column names from `data`. Only lower
            triangle is filled. If dropna =&#39;listwise&#39;, every correlation will have the same number of observations.
        summary : pandas.DataFrame
            Dataframe with columns [&#39;analysis&#39;, &#39;feature1&#39;, &#39;feature2&#39;, &#39;r-value&#39;, &#39;p-value&#39;, &#39;N&#39;, &#39;stat-sign&#39;]
            which indicate the type of test used for the correlation, the pair of columns, the correlation coefficient,
            the p-value, the number of observations for each combination of columns in `data` and whether the r-value is
            statistically significant.
    plotted_permuations: Figure
    Examples
    ----------
    &gt;&gt;&gt; from jmspack.frequentist_statistics import correlation_analysis
    &gt;&gt;&gt; import seaborn as sns
    &gt;&gt;&gt; iris = sns.load_dataset(&#39;iris&#39;)
    &gt;&gt;&gt; dict_results = correlation_analysis(iris, method=&#39;pearson&#39;, dropna=&#39;listwise&#39;, permutation_test=True,
    &gt;&gt;&gt;                                        n_permutations=100, check_norm=True)
    &gt;&gt;&gt; dict_results[&#39;summary&#39;]
    References
    ----------
    Bryant, C (2018). researchpy&#39;s documentation [Revision 9ae5ed63]. Retrieved from
    https://researchpy.readthedocs.io/en/latest/
    &#34;&#34;&#34;

    # Settings test
    if method == &#34;pearson&#34;:
        test, test_name = stats.pearsonr, &#34;Pearson&#34;
    elif method == &#34;spearman&#34;:
        test, test_name = stats.spearmanr, &#34;Spearman Rank&#34;
    elif method == &#34;kendall&#34;:
        test, test_name = stats.kendalltau, &#34;Kendall&#39;s Tau-b&#34;
    else:
        raise ValueError(&#34;method not in {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;}&#34;)

    # Copy numerical data from the original data
    data = data.copy().select_dtypes(&#34;number&#34;)

    # Get correct lists
    if col_list and not row_list:
        row_list = data.select_dtypes(&#34;number&#34;).drop(col_list, axis=1).columns.tolist()
    elif row_list and not col_list:
        col_list = data.select_dtypes(&#34;number&#34;).drop(row_list, axis=1).columns.tolist()

    # Initializing dataframes to store results
    info = pd.DataFrame()
    summary = pd.DataFrame()
    if not col_list and not row_list:
        r_vals = pd.DataFrame(columns=data.columns, index=data.columns)
        p_vals = pd.DataFrame(columns=data.columns, index=data.columns)
        n_vals = pd.DataFrame(columns=data.columns, index=data.columns)
        iterator = combinations(data.columns, 2)
    else:
        r_vals = pd.DataFrame(columns=col_list, index=row_list)
        p_vals = pd.DataFrame(columns=col_list, index=row_list)
        n_vals = pd.DataFrame(columns=col_list, index=row_list)
        iterator = product(col_list, row_list)

    if dropna == &#34;listwise&#34;:
        # Remove rows with missing values
        data = data.dropna(how=&#34;any&#34;, axis=&#34;index&#34;)
        info = info.append(
            {
                f&#34;{test_name} correlation test using {dropna} deletion&#34;: f&#34;Total observations used = {len(data)}&#34;
            },
            ignore_index=True,
        )
    elif dropna == &#34;pairwise&#34;:
        info = info.append(
            {
                f&#34;{test_name} correlation test using {dropna} deletion&#34;: f&#34;Observations in the data = {len(data)}&#34;
            },
            ignore_index=True,
        )
    else:
        raise ValueError(&#34;dropna not in {&#39;listwise&#39;, &#39;pairwise&#39;}&#34;)

    if check_norm:
        # Check normality of all columns in the data
        df_normality = normal_check(data)
        norm_names = df_normality.loc[df_normality[&#34;normality&#34;], &#34;feature&#34;].tolist()

    # Iterating through the Pandas series and performing the correlation
    for col1, col2 in iterator:
        if dropna == &#34;pairwise&#34;:
            # Remove rows with missing values in the pair of columns
            test_data = data[[col1, col2]].dropna()
        else:
            test_data = data

        if check_norm:
            # Select Pearson&#39;s r only if both columns are normally distributed
            if (col1 in norm_names) and (col2 in norm_names):
                test, test_name = stats.pearsonr, &#34;Pearson&#34;
            else:
                test, test_name = stats.spearmanr, &#34;Spearman Rank&#34;

        # Run correlations
        r_value, p_value = test(test_data.loc[:, col1], test_data.loc[:, col2])
        n_value = len(test_data)

        # Store output in matrix format
        try:
            r_vals.loc[col2, col1] = r_value
            p_vals.loc[col2, col1] = p_value
            n_vals.loc[col2, col1] = n_value
        except KeyError:
            r_vals.loc[col1, col2] = r_value
            p_vals.loc[col1, col2] = p_value
            n_vals.loc[col1, col2] = n_value

        # Store output in dataframe format
        dict_summary = {
            &#34;analysis&#34;: test_name,
            &#34;feature1&#34;: col1,
            &#34;feature2&#34;: col2,
            &#34;r-value&#34;: r_value,
            &#34;p-value&#34;: p_value,
            &#34;stat-sign&#34;: (p_value &lt; 0.05),
            &#34;N&#34;: n_value,
        }

        if permutation_test:
            raise ValueError(&#34;permutation_test has yet to be implemented&#34;)

            # # Copy the complete data
            # col2_shuffle = np.array(test_data.loc[:, col2])
            # col2_shuffle = np.repeat(
            #     col2_shuffle[:, np.newaxis], n_permutations, axis=1
            # )
            # # Shuffle within the columns
            # np.random.seed(random_state)
            # ix_i = np.random.sample(col2_shuffle.shape).argsort(axis=0)
            # ix_j = np.tile(np.arange(col2_shuffle.shape[1]), (col2_shuffle.shape[0], 1))
            # col2_shuffle = col2_shuffle[ix_i, ix_j]
            # permutations = np.apply_along_axis(
            #     permute_test,
            #     axis=0,
            #     arr=col2_shuffle,
            #     test_type=&#34;correlation&#34;,
            #     test=test,
            #     a2=np.array(test_data.loc[:, col1]),
            # )
            #
            # extreme_permutation = np.where(permutations &lt; p_value, 1, 0)
            # p_permutation = extreme_permutation.sum() / len(permutations)
            # dict_summary[&#34;permutation-p-value&#34;] = p_permutation
            #
            # # Reset random seed numpy
            # np.random.seed(None)

        summary = pd.concat(
            [summary, pd.DataFrame(data=dict_summary, index=[0])],
            axis=0,
            ignore_index=True,
            sort=False,
        )

    # Embed results within a dictionary
    result_dict = {
        &#34;r-value&#34;: r_vals,
        &#34;p-value&#34;: p_vals,
        &#34;N&#34;: n_vals,
        &#34;info&#34;: info,
        &#34;summary&#34;: summary,
    }

    return result_dict


def correlations_as_sample_increases(
    data: pd.DataFrame,
    feature1: str,
    feature2: str,
    starting_N: int = 10,
    step: int = 1,
    method=&#34;pearson&#34;,
    random_state=42,
    bootstrap: bool = False,
    bootstrap_per_N: int = 2,
    plot: bool = True,
    addition_to_title: str = &#34;&#34;,
    figsize: Tuple[float, float] = (9.0, 4.0),
    alpha: float = 0.05,
):
    r&#34;&#34;&#34;Plot changes in r-value and p-value from correlation between two features when sample size increases.
    Different methods to compute correlations are implemented. Data is shuffled first, to prevent any order effects.
    Parameters
    ----------
    data : pandas.DataFrame
        Dataframe with variables in columns, cases in rows
    feature1: str
        Name of column with first feature to be included in correlation
    feature2: str
        Name of column with second feature to be included in correlation
    starting_N: int (default: 10)
        Number of cases that should be used for first correlation
    step: int (default: 1)
        Step for increasing the number of cases for the correlations
    method: {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;}, default &#39;pearson&#39;
        Type of correlation, either Pearson&#39;s r, Spearman&#39;s rho, or Kendall&#39;s tau, implemented via respectively
        `scipy.stats.pearsonr`, `scipy.stats.spearmanr`, and `scipy.stats.kendalltau`.
    random_state: int (default: 42)
        Random state for reordering the data
    bootstrap: bool
        Whether to bootstrap the data at each N
    bootstrap_per_N: int
        If bootstrap is True then how many bootstraps per each sample size should be performed i.e if bootstrap_per_N
        is 2 then at sample size N=20, 2 bootstraps will be performed. This will continue until starting_N == N.
    plot: bool (default: True)
        Whether to plot the results
    addition_to_title: str (default: &#39;&#39;)
        The title of the plot will be &#34;The absolute r-value between {feature1} and {feature2} as N increases&#34; and
        followed by the addition (e.g. to describe a dataset).
    alpha: float (default: 0.05)
        Threshold for p-value that should be shown in the plot
    Returns
    ----------
    cor_results: pd.DataFrame
        Dataframe with the results for all ran analyses
    fig: Figure
        Figure will be returned if plot=True, otherwise None. This allows you to change properties of the figure
        afterwards, e.g. fig.axes[0].set_title(&#39;This is my new title&#39;)
    Examples
    ----------
    &gt;&gt;&gt; import seaborn as sns
    &gt;&gt;&gt; from jmspack.frequentist_statistics import correlations_as_sample_increases
    &gt;&gt;&gt; iris = sns.load_dataset(&#39;iris&#39;)
    &gt;&gt;&gt; summary,  fig = correlations_as_sample_increases(data=iris,feature1=&#39;petal_width&#39;,feature2=&#39;sepal_length&#39;,
    &gt;&gt;&gt; starting_N=20)
    &#34;&#34;&#34;

    data = (
        data[[feature1, feature2]].copy()
        # Remove rows with np.nans
        .dropna()
        # Randomize order of the data
        .sample(frac=1, random_state=random_state)
    )
    if data.shape[0] &lt; starting_N:
        raise ValueError(&#34;Number of valid cases is smaller than the starting_N&#34;)
    if data.shape[0] &lt; starting_N + step:
        raise ValueError(
            &#34;Number of valid cases is smaller than the starting_N + step (only one correlation possible)&#34;
        )

    # Initiate data frame for results
    corr_results = pd.DataFrame()

    # Loop through all possible number of rows from starting N till number of rows
    for i in range(starting_N, data.shape[0] + 1, step):
        boot_corr_results = pd.DataFrame()
        if bootstrap:
            for boot_num in range(0, bootstrap_per_N):
                boot_data = data.sample(frac=1, random_state=boot_num)
                current_boot_corr = correlation_analysis(
                    boot_data.iloc[0:i],
                    method=method,
                    check_norm=False,
                    permutation_test=False,
                )[&#34;summary&#34;][[&#34;r-value&#34;, &#34;p-value&#34;, &#34;N&#34;]]
                boot_corr_results = pd.concat(
                    [boot_corr_results, current_boot_corr], ignore_index=True
                )
            corr_results = pd.concat(
                [corr_results, boot_corr_results], ignore_index=True
            )
        else:
            # Run correlation with all data from first row until row i
            current_corr = correlation_analysis(
                data.iloc[0:i], method=method, check_norm=False, permutation_test=False
            )[&#34;summary&#34;][[&#34;r-value&#34;, &#34;p-value&#34;, &#34;N&#34;]]
            corr_results = pd.concat([corr_results, current_corr], ignore_index=True)

    fig = None
    if plot:
        fig, ax = plt.subplots(figsize=figsize)
        # Add r-value and p-value
        _ = sns.lineplot(
            x=corr_results[&#34;N&#34;],
            y=abs(corr_results[&#34;r-value&#34;]),
            label=&#34;absolute r-value&#34;,
            ax=ax,
        ).set_title(
            f&#34;The absolute r-value between {feature1} and {feature2}\nas N increases {addition_to_title}&#34;
        )
        _ = sns.lineplot(
            x=corr_results[&#34;N&#34;], y=corr_results[&#34;p-value&#34;], label=&#34;p-value&#34;, ax=ax
        )
        # Add alpha level (threshold for p-value)
        _ = ax.axhline(
            y=alpha, color=&#34;black&#34;, alpha=0.5, linestyle=&#34;--&#34;, label=f&#34;&gt;= {alpha}&#34;
        )

        _ = ax.set_ylabel(&#34;&#34;)
        _ = ax.set_ylim(0, 1)
        _ = plt.legend()
    return corr_results, fig


def multiple_univariate_OLSs(
    X: pd.DataFrame,
    y: pd.Series,
    features_list: list,
):
    all_coefs_df = pd.DataFrame()
    for feature in features_list:
        mod = sm.OLS(endog=y, exog=sm.add_constant(X[[feature]]))
        res = mod.fit()
        coef_df = pd.read_html(
            res.summary().tables[1].as_html(), header=0, index_col=0
        )[0].drop(&#34;const&#34;)
        coef_df = coef_df.assign(
            **{&#34;rsquared&#34;: res.rsquared, &#34;rsquared_adj&#34;: res.rsquared_adj}
        )
        all_coefs_df = pd.concat([all_coefs_df, coef_df])
    return all_coefs_df


def potential_for_change_index(
    data: pd.DataFrame,
    features_list: list,
    target: str,
    minimum_measure: str = &#34;min&#34;,
    centrality_measure: str = &#34;mean&#34;,
    maximum_measure: str = &#34;max&#34;,
    weight_measure: str = &#34;rsquared_adj&#34;,
    scale_data: bool = True,
    pci_heatmap: bool = True,
    pci_heatmap_figsize: Tuple[float, float] = (1.0, 4.0),
):
    if scale_data:
        data = data.pipe(apply_scaling)

    if weight_measure == &#34;rsquared_adj&#34; or weight_measure == &#34;rsquared&#34;:
        tmp_X = data[features_list]
        tmp_y = data[target]
        weight_df = multiple_univariate_OLSs(
            X=tmp_X, y=tmp_y, features_list=features_list
        )

        negative_list = weight_df[weight_df[&#34;coef&#34;] &lt; 0].index.tolist()

    else:
        output_dict = correlation_analysis(
            data=data,
            col_list=features_list,
            row_list=[target],
            method=&#34;pearson&#34;,
            check_norm=False,
            dropna=&#34;pairwise&#34;,
            permutation_test=False,
            n_permutations=10,
            random_state=69420,
        )
        weight_df = output_dict[&#34;summary&#34;].set_index(&#34;feature1&#34;)
        negative_list = weight_df[weight_df[&#34;r-value&#34;] &lt; 0].index.tolist()

    if len(negative_list) &lt; 0:

        pci_df = (
            # room for improvement calculation (series)
            (
                data[features_list].agg(centrality_measure)
                - (data[features_list].agg(maximum_measure))
            ).abs()
            * weight_df[weight_measure]  # weight (based on weight_measure series)
        ).to_frame(&#34;PCI&#34;)
    else:
        neg_pci_df = (
            # room for improvement calculation (series)
            (
                data[negative_list].agg(centrality_measure)
                - (data[negative_list].agg(minimum_measure))
            ).abs()
            * weight_df.loc[
                negative_list, weight_measure
            ]  # weight (based on weight_measure series)
        ).to_frame(&#34;PCI&#34;)

        pos_pci_df = (
            # room for improvement calculation (series)
            (
                data[features_list].drop(negative_list, axis=1).agg(centrality_measure)
                - (data[features_list].drop(negative_list, axis=1).agg(maximum_measure))
            ).abs()
            * weight_df[weight_measure].drop(
                negative_list, axis=0
            )  # weight (based on weight_measure series)
        ).to_frame(&#34;PCI&#34;)

        pci_df = pd.concat([pos_pci_df, neg_pci_df])

    if pci_heatmap:
        _ = plt.figure(figsize=pci_heatmap_figsize)
        _ = sns.heatmap(
            data=pci_df.sort_values(by=&#34;PCI&#34;, ascending=False),
            annot=True,
            fmt=&#34;.3g&#34;,
        )

    if weight_measure == &#34;rsquared_adj&#34; or weight_measure == &#34;rsquared&#34;:
        return pci_df.merge(
            data[features_list]
            .agg([minimum_measure, centrality_measure, maximum_measure])
            .T,
            left_index=True,
            right_index=True,
        ).merge(weight_df[[weight_measure, &#34;P&gt;|t|&#34;]], left_index=True, right_index=True)

    else:
        return pci_df.merge(
            data[features_list]
            .agg([minimum_measure, centrality_measure, maximum_measure])
            .T,
            left_index=True,
            right_index=True,
        ).merge(
            weight_df.loc[:, [weight_measure, &#34;p-value&#34;]],
            left_index=True,
            right_index=True,
        )


def correct_pvalues(
    pvals,
    alpha: float = 0.05,
    method: str = &#34;fdr_bh&#34;,
    plot: bool = False,
    labels=None,
    title: str = &#34;&#34;,
    figsize: tuple = (10, 5),
):
    r&#34;&#34;&#34;
    Correct an array-like with pvalues using `method`, wrapper for `statsmodels.stats.multitest.multipletests`
    Parameters
    ----------
    pvals: array-like, 1d
        uncorrected pvalues
    alpha: float
        FWER, family-wise error rate
    method: str, one of {&#39;bonferroni&#39;, &#39;sidak&#39;, &#39;holm-sidak&#39;, &#39;holm&#39;, &#39;simes-hochberg&#39;, &#39;hommel&#39;, &#39;fdr_bh&#39;,
    &#39;fdr_by&#39;, &#39;fdr_tsbh&#39;, &#39;fdr_tsbky&#39;}
    plot: bool
        whether to plot the results
    title: str
        title to show above the plot
    labels: array-like, 1d
        labels for the uncorrected pvalues
    figsize: tuple
        size for the Figure
    Returns
    ----------
    reject: numpy.array, bool
        true for hypothesis that can be rejected for given alpha
    corrected_p: numpy.array
        p-values corrected for multiple tests
    pvalues_plot: matplotlib.figure.Figure (optional)
        Figure if plot == True, else None
    &#34;&#34;&#34;

    if isinstance(pvals, pd.Series):
        pvals = pvals.values

    if labels is not None:
        if len(pvals) != len(labels):
            raise ValueError(&#34;Lengths of the pvals and the pvals_labels does not match&#34;)
        if isinstance(labels, pd.Series):
            labels = labels.values

    reject, corrected_p, _, _ = multipletests(
        pvals=pvals, alpha=alpha, method=method, returnsorted=True
    )

    # Sort the pvalues and the labels (correct pvalues are sorted already)
    sort_order = pvals.argsort()
    pvals = pvals[sort_order]
    if labels is not None:
        labels = labels[sort_order]
        labels = np.insert(labels, [0], [&#34;&#34;])

    # Get colors for all pvalues
    colors = [&#34;#2167C5&#34; if i else &#34;#EB5E23&#34; for i in reject]

    pvalues_plot = None

    if plot:
        pvalues_plot, ax = plt.subplots(figsize=figsize)
        x = 1
        # Plot pvalues and corrected pvalues, color dependent on &#39;reject&#39;
        for p, cp, c in zip(pvals, corrected_p, colors):
            _ = plt.plot(x, p, &#34;o&#34;, c=c)
            _ = plt.plot(x, cp, &#34;x&#34;, c=c)
            x += 1

        # Variable for pvalues length and number of pvalue
        n = len(pvals)
        i = np.arange(n) + 1

        # Plot line at familywise p value
        familywise_p = np.repeat(alpha, n)
        _ = plt.plot(i, familywise_p, &#34;k--&#34;)

        # Add legend elements
        legend_elements = [
            Line2D(
                [0],
                [0],
                marker=&#34;o&#34;,
                color=&#34;k&#34;,
                label=&#34;Original p-values&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0],
                [0],
                marker=&#34;x&#34;,
                color=&#34;k&#34;,
                label=&#34;Corrected p-values&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0],
                [0],
                marker=&#34;X&#34;,
                color=&#34;#EB5E23&#34;,
                label=&#34;Non-significant&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0],
                [0],
                marker=&#34;X&#34;,
                color=&#34;#2167C5&#34;,
                label=&#34;Significant&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0], [0], marker=&#34;&#34;, color=&#34;k&#34;, label=f&#34;Alpha = {alpha}&#34;, linestyle=&#34;--&#34;
            ),
        ]

        if method == &#34;fdr_bh&#34;:
            # Plot a diagonal line to show the boundary pvalue
            optimum_p = alpha * i / n
            _ = plt.plot(i, optimum_p, &#34;k-&#34;)
            legend_elements.append(
                Line2D(
                    [0],
                    [0],
                    marker=&#34;&#34;,
                    color=&#34;k&#34;,
                    label=&#34;Benjamini-Hochberg decision line&#34;,
                    linestyle=&#34;-&#34;,
                )
            )

        # Add labels and legend
        _ = plt.xlabel(&#34;$i$&#34;)
        _ = plt.ylabel(&#34;$p$&#34;)
        _ = plt.title(title)

        _ = ax.legend(handles=legend_elements)

        if labels is not None:
            _ = plt.xticks(plt.xticks()[0], labels)

    return reject, corrected_p, pvalues_plot


def partial_correlation(df: pd.DataFrame):
    &#34;&#34;&#34;
    Returns the sample linear partial correlation coefficients between pairs of variables,
    controlling for all other remaining variables
    Parameters
    ----------
    df : array-like, shape (n, p)
        Array with the different variables. Each column is taken as a variable.
    Returns
    -------
    P : array-like, shape (p, p)
        P[i, j] contains the partial correlation of input_df[:, i] and input_df[:, j]
        controlling for all other remaining variables.
    &#34;&#34;&#34;
    partial_corr_matrix_rvals = np.zeros((df.shape[1], df.shape[1]))
    partial_corr_matrix_pvals = np.zeros((df.shape[1], df.shape[1]))

    for i, column1 in enumerate(df):
        for j, column2 in enumerate(df):
            control_variables = np.delete(np.arange(df.shape[1]), [i, j])
            if i == j:
                partial_corr_matrix_rvals[i, j] = 1
                partial_corr_matrix_pvals[i, j] = 1
                continue
            data_control_variable = df.iloc[:, control_variables]
            data_column1 = df[column1].values
            data_column2 = df[column2].values
            fit1 = LinearRegression(fit_intercept=True)
            fit2 = LinearRegression(fit_intercept=True)
            fit1.fit(data_control_variable, data_column1)
            fit2.fit(data_control_variable, data_column2)
            residual1 = data_column1 - (
                np.dot(data_control_variable, fit1.coef_) + fit1.intercept_
            )
            residual2 = data_column2 - (
                np.dot(data_control_variable, fit2.coef_) + fit2.intercept_
            )
            partial_corr_matrix_rvals[i, j] = stats.pearsonr(residual1, residual2)[0]
            partial_corr_matrix_pvals[i, j] = stats.pearsonr(residual1, residual2)[1]
            partial_corr_matrix_rvals_df = pd.DataFrame(
                partial_corr_matrix_rvals, columns=df.columns, index=df.columns
            )
            partial_corr_matrix_pvals_df = pd.DataFrame(
                partial_corr_matrix_pvals, columns=df.columns, index=df.columns
            )

    return partial_corr_matrix_rvals_df, partial_corr_matrix_pvals_df</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="jmspack.frequentist_statistics.correct_pvalues"><code class="name flex">
<span>def <span class="ident">correct_pvalues</span></span>(<span>pvals, alpha: float = 0.05, method: str = 'fdr_bh', plot: bool = False, labels=None, title: str = '', figsize: tuple = (10, 5))</span>
</code></dt>
<dd>
<div class="desc"><p>Correct an array-like with pvalues using <code>method</code>, wrapper for <code>statsmodels.stats.multitest.multipletests</code>
Parameters</p>
<hr>
<dl>
<dt><strong><code>pvals</code></strong> :&ensp;<code>array-like, 1d</code></dt>
<dd>uncorrected pvalues</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>FWER, family-wise error rate</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str, one</code> of <code>{'bonferroni', 'sidak', 'holm-sidak', 'holm', 'simes-hochberg', 'hommel', 'fdr_bh',</code></dt>
<dd>&nbsp;</dd>
<dt>'fdr_by', 'fdr_tsbh', 'fdr_tsbky'}</dt>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to plot the results</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code></dt>
<dd>title to show above the plot</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>array-like, 1d</code></dt>
<dd>labels for the uncorrected pvalues</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code></dt>
<dd>size for the Figure</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>reject</code></strong> :&ensp;<code>numpy.array, bool</code></dt>
<dd>true for hypothesis that can be rejected for given alpha</dd>
<dt><strong><code>corrected_p</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>p-values corrected for multiple tests</dd>
<dt><strong><code>pvalues_plot</code></strong> :&ensp;<code>matplotlib.figure.Figure (optional)</code></dt>
<dd>Figure if plot == True, else None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correct_pvalues(
    pvals,
    alpha: float = 0.05,
    method: str = &#34;fdr_bh&#34;,
    plot: bool = False,
    labels=None,
    title: str = &#34;&#34;,
    figsize: tuple = (10, 5),
):
    r&#34;&#34;&#34;
    Correct an array-like with pvalues using `method`, wrapper for `statsmodels.stats.multitest.multipletests`
    Parameters
    ----------
    pvals: array-like, 1d
        uncorrected pvalues
    alpha: float
        FWER, family-wise error rate
    method: str, one of {&#39;bonferroni&#39;, &#39;sidak&#39;, &#39;holm-sidak&#39;, &#39;holm&#39;, &#39;simes-hochberg&#39;, &#39;hommel&#39;, &#39;fdr_bh&#39;,
    &#39;fdr_by&#39;, &#39;fdr_tsbh&#39;, &#39;fdr_tsbky&#39;}
    plot: bool
        whether to plot the results
    title: str
        title to show above the plot
    labels: array-like, 1d
        labels for the uncorrected pvalues
    figsize: tuple
        size for the Figure
    Returns
    ----------
    reject: numpy.array, bool
        true for hypothesis that can be rejected for given alpha
    corrected_p: numpy.array
        p-values corrected for multiple tests
    pvalues_plot: matplotlib.figure.Figure (optional)
        Figure if plot == True, else None
    &#34;&#34;&#34;

    if isinstance(pvals, pd.Series):
        pvals = pvals.values

    if labels is not None:
        if len(pvals) != len(labels):
            raise ValueError(&#34;Lengths of the pvals and the pvals_labels does not match&#34;)
        if isinstance(labels, pd.Series):
            labels = labels.values

    reject, corrected_p, _, _ = multipletests(
        pvals=pvals, alpha=alpha, method=method, returnsorted=True
    )

    # Sort the pvalues and the labels (correct pvalues are sorted already)
    sort_order = pvals.argsort()
    pvals = pvals[sort_order]
    if labels is not None:
        labels = labels[sort_order]
        labels = np.insert(labels, [0], [&#34;&#34;])

    # Get colors for all pvalues
    colors = [&#34;#2167C5&#34; if i else &#34;#EB5E23&#34; for i in reject]

    pvalues_plot = None

    if plot:
        pvalues_plot, ax = plt.subplots(figsize=figsize)
        x = 1
        # Plot pvalues and corrected pvalues, color dependent on &#39;reject&#39;
        for p, cp, c in zip(pvals, corrected_p, colors):
            _ = plt.plot(x, p, &#34;o&#34;, c=c)
            _ = plt.plot(x, cp, &#34;x&#34;, c=c)
            x += 1

        # Variable for pvalues length and number of pvalue
        n = len(pvals)
        i = np.arange(n) + 1

        # Plot line at familywise p value
        familywise_p = np.repeat(alpha, n)
        _ = plt.plot(i, familywise_p, &#34;k--&#34;)

        # Add legend elements
        legend_elements = [
            Line2D(
                [0],
                [0],
                marker=&#34;o&#34;,
                color=&#34;k&#34;,
                label=&#34;Original p-values&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0],
                [0],
                marker=&#34;x&#34;,
                color=&#34;k&#34;,
                label=&#34;Corrected p-values&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0],
                [0],
                marker=&#34;X&#34;,
                color=&#34;#EB5E23&#34;,
                label=&#34;Non-significant&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0],
                [0],
                marker=&#34;X&#34;,
                color=&#34;#2167C5&#34;,
                label=&#34;Significant&#34;,
                linestyle=&#34;none&#34;,
            ),
            Line2D(
                [0], [0], marker=&#34;&#34;, color=&#34;k&#34;, label=f&#34;Alpha = {alpha}&#34;, linestyle=&#34;--&#34;
            ),
        ]

        if method == &#34;fdr_bh&#34;:
            # Plot a diagonal line to show the boundary pvalue
            optimum_p = alpha * i / n
            _ = plt.plot(i, optimum_p, &#34;k-&#34;)
            legend_elements.append(
                Line2D(
                    [0],
                    [0],
                    marker=&#34;&#34;,
                    color=&#34;k&#34;,
                    label=&#34;Benjamini-Hochberg decision line&#34;,
                    linestyle=&#34;-&#34;,
                )
            )

        # Add labels and legend
        _ = plt.xlabel(&#34;$i$&#34;)
        _ = plt.ylabel(&#34;$p$&#34;)
        _ = plt.title(title)

        _ = ax.legend(handles=legend_elements)

        if labels is not None:
            _ = plt.xticks(plt.xticks()[0], labels)

    return reject, corrected_p, pvalues_plot</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.correlation_analysis"><code class="name flex">
<span>def <span class="ident">correlation_analysis</span></span>(<span>data: pandas.core.frame.DataFrame, col_list=None, row_list=None, check_norm=False, method: str = 'pearson', dropna: str = 'pairwise', permutation_test: bool = False, n_permutations: int = 1000, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Run correlations for numerical features and return output in different formats
Different methods to compute correlations and to handle missing values are implemented.
Inspired by <code>researchpy.corr_case</code> and <code>researchpy.corr_pair</code>.
Parameters</p>
<hr>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with variables in columns, cases in rows</dd>
<dt><strong><code>row_list</code></strong> :&ensp;<code>list</code> or <code>None (default: None)</code></dt>
<dd>List with names of columns in <code>data</code> that should be in the rows of the correlogram.
If None, all columns are used but only every unique combination.</dd>
<dt><strong><code>col_list</code></strong> :&ensp;<code>list</code> or <code>None (default: None)</code></dt>
<dd>List with names of columns in <code>data</code> that should be in the columns of the correlogram.
If None, all columns are used and only every unique combination.</dd>
<dt><strong><code>check_norm</code></strong> :&ensp;<code>bool (default: False)</code></dt>
<dd>If True, normality will be checked for columns in <code>data</code> using <code><a title="jmspack.frequentist_statistics.normal_check" href="#jmspack.frequentist_statistics.normal_check">normal_check()</a></code>. This influences the used method
for correlations, i.e. Pearson or Spearman. Note: normality check ignores missing values.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'pearson', 'kendall', 'spearman'}</code>, default <code>'pearson'</code></dt>
<dd>Type of correlation, either Pearson's r, Spearman's rho, or Kendall's tau, implemented via respectively
<code>scipy.stats.pearsonr</code>, <code>scipy.stats.spearmanr</code>, and <code>scipy.stats.kendalltau</code>
Will be ignored if check_norm=True. Instead, Person's r is used for every combination of normally distributed
columns and Spearman's rho is used for all other combinations.</dd>
<dt><strong><code>dropna</code></strong> :&ensp;<code>{'listwise', 'pairwise'}</code>, default <code>'pairwise'</code></dt>
<dd>Should rows with missing values be dropped over the complete <code>data</code> ('listwise') or for every correlation
separately ('pairwise')</dd>
<dt><strong><code>permutation_test</code></strong> :&ensp;<code>bool (default: False)</code></dt>
<dd>If true, a permutation test will added</dd>
<dt><strong><code>n_permutations</code></strong> :&ensp;<code>int (default: 1000)</code></dt>
<dd>Number of permutations in the permutation test</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>None</code> or <code>int (default: None)</code></dt>
<dd>Random state for permutation_test. If not None, random_state will be updated for every permutation</dd>
<dt><strong><code>plot_permutation</code></strong> :&ensp;<code>bool (default: False)</code></dt>
<dd>Whether to plot the results of the permutation test</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple (default: (11.7, 8.27))</code></dt>
<dd>Width and height of the figure in inches</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>result_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing with the following keys:
info : pandas.DataFrame
Description of correlation method, missing values handling and number of observations
r-values : pandas.DataFrame
Dataframe with correlation coefficients. Indices and columns are column names from <code>data</code>. Only lower
triangle is filled.
p-values : pandas.DataFrame
Dataframe with p-values. Indices and columns are column names from <code>data</code>. Only lower triangle is filled.
N
: pandas.DataFrame
Dataframe with numbers of observations. Indices and columns are column names from <code>data</code>. Only lower
triangle is filled. If dropna ='listwise', every correlation will have the same number of observations.
summary : pandas.DataFrame
Dataframe with columns ['analysis', 'feature1', 'feature2', 'r-value', 'p-value', 'N', 'stat-sign']
which indicate the type of test used for the correlation, the pair of columns, the correlation coefficient,
the p-value, the number of observations for each combination of columns in <code>data</code> and whether the r-value is
statistically significant.</dd>
<dt><strong><code>plotted_permuations</code></strong> :&ensp;<code>Figure</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">&gt;&gt;&gt; from jmspack.frequentist_statistics import correlation_analysis
&gt;&gt;&gt; import seaborn as sns
&gt;&gt;&gt; iris = sns.load_dataset('iris')
&gt;&gt;&gt; dict_results = correlation_analysis(iris, method='pearson', dropna='listwise', permutation_test=True,
&gt;&gt;&gt;                                        n_permutations=100, check_norm=True)
&gt;&gt;&gt; dict_results['summary']
References
-----
Bryant, C (2018). researchpy's documentation [Revision 9ae5ed63]. Retrieved from
https://researchpy.readthedocs.io/en/latest/
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correlation_analysis(
    data: pd.DataFrame,
    col_list=None,
    row_list=None,
    check_norm=False,
    method: str = &#34;pearson&#34;,
    dropna: str = &#34;pairwise&#34;,
    permutation_test: bool = False,
    n_permutations: int = 1000,
    random_state=None,
):
    r&#34;&#34;&#34;Run correlations for numerical features and return output in different formats
    Different methods to compute correlations and to handle missing values are implemented.
    Inspired by `researchpy.corr_case` and `researchpy.corr_pair`.
    Parameters
    ----------
    data : pandas.DataFrame
        Dataframe with variables in columns, cases in rows
    row_list: list or None (default: None)
        List with names of columns in `data` that should be in the rows of the correlogram.
        If None, all columns are used but only every unique combination.
    col_list: list or None (default: None)
        List with names of columns in `data` that should be in the columns of the correlogram.
        If None, all columns are used and only every unique combination.
    check_norm: bool (default: False)
        If True, normality will be checked for columns in `data` using `normal_check`. This influences the used method
        for correlations, i.e. Pearson or Spearman. Note: normality check ignores missing values.
    method: {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;}, default &#39;pearson&#39;
        Type of correlation, either Pearson&#39;s r, Spearman&#39;s rho, or Kendall&#39;s tau, implemented via respectively
        `scipy.stats.pearsonr`, `scipy.stats.spearmanr`, and `scipy.stats.kendalltau`
        Will be ignored if check_norm=True. Instead, Person&#39;s r is used for every combination of normally distributed
        columns and Spearman&#39;s rho is used for all other combinations.
    dropna : {&#39;listwise&#39;, &#39;pairwise&#39;}, default &#39;pairwise&#39;
        Should rows with missing values be dropped over the complete `data` (&#39;listwise&#39;) or for every correlation
        separately (&#39;pairwise&#39;)
    permutation_test: bool (default: False)
        If true, a permutation test will added
    n_permutations: int (default: 1000)
        Number of permutations in the permutation test
    random_state: None or int (default: None)
        Random state for permutation_test. If not None, random_state will be updated for every permutation
    plot_permutation: bool (default: False)
        Whether to plot the results of the permutation test
    figsize: tuple (default: (11.7, 8.27))
        Width and height of the figure in inches
    Returns
    ----------
    result_dict: dict
        Dictionary containing with the following keys:
        info : pandas.DataFrame
            Description of correlation method, missing values handling and number of observations
        r-values : pandas.DataFrame
            Dataframe with correlation coefficients. Indices and columns are column names from `data`. Only lower
            triangle is filled.
        p-values : pandas.DataFrame
            Dataframe with p-values. Indices and columns are column names from `data`. Only lower triangle is filled.
        N        : pandas.DataFrame
            Dataframe with numbers of observations. Indices and columns are column names from `data`. Only lower
            triangle is filled. If dropna =&#39;listwise&#39;, every correlation will have the same number of observations.
        summary : pandas.DataFrame
            Dataframe with columns [&#39;analysis&#39;, &#39;feature1&#39;, &#39;feature2&#39;, &#39;r-value&#39;, &#39;p-value&#39;, &#39;N&#39;, &#39;stat-sign&#39;]
            which indicate the type of test used for the correlation, the pair of columns, the correlation coefficient,
            the p-value, the number of observations for each combination of columns in `data` and whether the r-value is
            statistically significant.
    plotted_permuations: Figure
    Examples
    ----------
    &gt;&gt;&gt; from jmspack.frequentist_statistics import correlation_analysis
    &gt;&gt;&gt; import seaborn as sns
    &gt;&gt;&gt; iris = sns.load_dataset(&#39;iris&#39;)
    &gt;&gt;&gt; dict_results = correlation_analysis(iris, method=&#39;pearson&#39;, dropna=&#39;listwise&#39;, permutation_test=True,
    &gt;&gt;&gt;                                        n_permutations=100, check_norm=True)
    &gt;&gt;&gt; dict_results[&#39;summary&#39;]
    References
    ----------
    Bryant, C (2018). researchpy&#39;s documentation [Revision 9ae5ed63]. Retrieved from
    https://researchpy.readthedocs.io/en/latest/
    &#34;&#34;&#34;

    # Settings test
    if method == &#34;pearson&#34;:
        test, test_name = stats.pearsonr, &#34;Pearson&#34;
    elif method == &#34;spearman&#34;:
        test, test_name = stats.spearmanr, &#34;Spearman Rank&#34;
    elif method == &#34;kendall&#34;:
        test, test_name = stats.kendalltau, &#34;Kendall&#39;s Tau-b&#34;
    else:
        raise ValueError(&#34;method not in {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;}&#34;)

    # Copy numerical data from the original data
    data = data.copy().select_dtypes(&#34;number&#34;)

    # Get correct lists
    if col_list and not row_list:
        row_list = data.select_dtypes(&#34;number&#34;).drop(col_list, axis=1).columns.tolist()
    elif row_list and not col_list:
        col_list = data.select_dtypes(&#34;number&#34;).drop(row_list, axis=1).columns.tolist()

    # Initializing dataframes to store results
    info = pd.DataFrame()
    summary = pd.DataFrame()
    if not col_list and not row_list:
        r_vals = pd.DataFrame(columns=data.columns, index=data.columns)
        p_vals = pd.DataFrame(columns=data.columns, index=data.columns)
        n_vals = pd.DataFrame(columns=data.columns, index=data.columns)
        iterator = combinations(data.columns, 2)
    else:
        r_vals = pd.DataFrame(columns=col_list, index=row_list)
        p_vals = pd.DataFrame(columns=col_list, index=row_list)
        n_vals = pd.DataFrame(columns=col_list, index=row_list)
        iterator = product(col_list, row_list)

    if dropna == &#34;listwise&#34;:
        # Remove rows with missing values
        data = data.dropna(how=&#34;any&#34;, axis=&#34;index&#34;)
        info = info.append(
            {
                f&#34;{test_name} correlation test using {dropna} deletion&#34;: f&#34;Total observations used = {len(data)}&#34;
            },
            ignore_index=True,
        )
    elif dropna == &#34;pairwise&#34;:
        info = info.append(
            {
                f&#34;{test_name} correlation test using {dropna} deletion&#34;: f&#34;Observations in the data = {len(data)}&#34;
            },
            ignore_index=True,
        )
    else:
        raise ValueError(&#34;dropna not in {&#39;listwise&#39;, &#39;pairwise&#39;}&#34;)

    if check_norm:
        # Check normality of all columns in the data
        df_normality = normal_check(data)
        norm_names = df_normality.loc[df_normality[&#34;normality&#34;], &#34;feature&#34;].tolist()

    # Iterating through the Pandas series and performing the correlation
    for col1, col2 in iterator:
        if dropna == &#34;pairwise&#34;:
            # Remove rows with missing values in the pair of columns
            test_data = data[[col1, col2]].dropna()
        else:
            test_data = data

        if check_norm:
            # Select Pearson&#39;s r only if both columns are normally distributed
            if (col1 in norm_names) and (col2 in norm_names):
                test, test_name = stats.pearsonr, &#34;Pearson&#34;
            else:
                test, test_name = stats.spearmanr, &#34;Spearman Rank&#34;

        # Run correlations
        r_value, p_value = test(test_data.loc[:, col1], test_data.loc[:, col2])
        n_value = len(test_data)

        # Store output in matrix format
        try:
            r_vals.loc[col2, col1] = r_value
            p_vals.loc[col2, col1] = p_value
            n_vals.loc[col2, col1] = n_value
        except KeyError:
            r_vals.loc[col1, col2] = r_value
            p_vals.loc[col1, col2] = p_value
            n_vals.loc[col1, col2] = n_value

        # Store output in dataframe format
        dict_summary = {
            &#34;analysis&#34;: test_name,
            &#34;feature1&#34;: col1,
            &#34;feature2&#34;: col2,
            &#34;r-value&#34;: r_value,
            &#34;p-value&#34;: p_value,
            &#34;stat-sign&#34;: (p_value &lt; 0.05),
            &#34;N&#34;: n_value,
        }

        if permutation_test:
            raise ValueError(&#34;permutation_test has yet to be implemented&#34;)

            # # Copy the complete data
            # col2_shuffle = np.array(test_data.loc[:, col2])
            # col2_shuffle = np.repeat(
            #     col2_shuffle[:, np.newaxis], n_permutations, axis=1
            # )
            # # Shuffle within the columns
            # np.random.seed(random_state)
            # ix_i = np.random.sample(col2_shuffle.shape).argsort(axis=0)
            # ix_j = np.tile(np.arange(col2_shuffle.shape[1]), (col2_shuffle.shape[0], 1))
            # col2_shuffle = col2_shuffle[ix_i, ix_j]
            # permutations = np.apply_along_axis(
            #     permute_test,
            #     axis=0,
            #     arr=col2_shuffle,
            #     test_type=&#34;correlation&#34;,
            #     test=test,
            #     a2=np.array(test_data.loc[:, col1]),
            # )
            #
            # extreme_permutation = np.where(permutations &lt; p_value, 1, 0)
            # p_permutation = extreme_permutation.sum() / len(permutations)
            # dict_summary[&#34;permutation-p-value&#34;] = p_permutation
            #
            # # Reset random seed numpy
            # np.random.seed(None)

        summary = pd.concat(
            [summary, pd.DataFrame(data=dict_summary, index=[0])],
            axis=0,
            ignore_index=True,
            sort=False,
        )

    # Embed results within a dictionary
    result_dict = {
        &#34;r-value&#34;: r_vals,
        &#34;p-value&#34;: p_vals,
        &#34;N&#34;: n_vals,
        &#34;info&#34;: info,
        &#34;summary&#34;: summary,
    }

    return result_dict</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.correlations_as_sample_increases"><code class="name flex">
<span>def <span class="ident">correlations_as_sample_increases</span></span>(<span>data: pandas.core.frame.DataFrame, feature1: str, feature2: str, starting_N: int = 10, step: int = 1, method='pearson', random_state=42, bootstrap: bool = False, bootstrap_per_N: int = 2, plot: bool = True, addition_to_title: str = '', figsize: Tuple[float, float] = (9.0, 4.0), alpha: float = 0.05)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot changes in r-value and p-value from correlation between two features when sample size increases.
Different methods to compute correlations are implemented. Data is shuffled first, to prevent any order effects.
Parameters</p>
<hr>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with variables in columns, cases in rows</dd>
<dt><strong><code>feature1</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of column with first feature to be included in correlation</dd>
<dt><strong><code>feature2</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of column with second feature to be included in correlation</dd>
<dt><strong><code>starting_N</code></strong> :&ensp;<code>int (default: 10)</code></dt>
<dd>Number of cases that should be used for first correlation</dd>
<dt><strong><code>step</code></strong> :&ensp;<code>int (default: 1)</code></dt>
<dd>Step for increasing the number of cases for the correlations</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'pearson', 'kendall', 'spearman'}</code>, default <code>'pearson'</code></dt>
<dd>Type of correlation, either Pearson's r, Spearman's rho, or Kendall's tau, implemented via respectively
<code>scipy.stats.pearsonr</code>, <code>scipy.stats.spearmanr</code>, and <code>scipy.stats.kendalltau</code>.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int (default: 42)</code></dt>
<dd>Random state for reordering the data</dd>
<dt><strong><code>bootstrap</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to bootstrap the data at each N</dd>
<dt><strong><code>bootstrap_per_N</code></strong> :&ensp;<code>int</code></dt>
<dd>If bootstrap is True then how many bootstraps per each sample size should be performed i.e if bootstrap_per_N
is 2 then at sample size N=20, 2 bootstraps will be performed. This will continue until starting_N == N.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool (default: True)</code></dt>
<dd>Whether to plot the results</dd>
<dt><strong><code>addition_to_title</code></strong> :&ensp;<code>str (default: '')</code></dt>
<dd>The title of the plot will be "The absolute r-value between {feature1} and {feature2} as N increases" and
followed by the addition (e.g. to describe a dataset).</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float (default: 0.05)</code></dt>
<dd>Threshold for p-value that should be shown in the plot</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cor_results</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe with the results for all ran analyses</dd>
<dt><strong><code>fig</code></strong> :&ensp;<code>Figure</code></dt>
<dd>Figure will be returned if plot=True, otherwise None. This allows you to change properties of the figure
afterwards, e.g. fig.axes[0].set_title('This is my new title')</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">&gt;&gt;&gt; import seaborn as sns
&gt;&gt;&gt; from jmspack.frequentist_statistics import correlations_as_sample_increases
&gt;&gt;&gt; iris = sns.load_dataset('iris')
&gt;&gt;&gt; summary,  fig = correlations_as_sample_increases(data=iris,feature1='petal_width',feature2='sepal_length',
&gt;&gt;&gt; starting_N=20)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correlations_as_sample_increases(
    data: pd.DataFrame,
    feature1: str,
    feature2: str,
    starting_N: int = 10,
    step: int = 1,
    method=&#34;pearson&#34;,
    random_state=42,
    bootstrap: bool = False,
    bootstrap_per_N: int = 2,
    plot: bool = True,
    addition_to_title: str = &#34;&#34;,
    figsize: Tuple[float, float] = (9.0, 4.0),
    alpha: float = 0.05,
):
    r&#34;&#34;&#34;Plot changes in r-value and p-value from correlation between two features when sample size increases.
    Different methods to compute correlations are implemented. Data is shuffled first, to prevent any order effects.
    Parameters
    ----------
    data : pandas.DataFrame
        Dataframe with variables in columns, cases in rows
    feature1: str
        Name of column with first feature to be included in correlation
    feature2: str
        Name of column with second feature to be included in correlation
    starting_N: int (default: 10)
        Number of cases that should be used for first correlation
    step: int (default: 1)
        Step for increasing the number of cases for the correlations
    method: {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;}, default &#39;pearson&#39;
        Type of correlation, either Pearson&#39;s r, Spearman&#39;s rho, or Kendall&#39;s tau, implemented via respectively
        `scipy.stats.pearsonr`, `scipy.stats.spearmanr`, and `scipy.stats.kendalltau`.
    random_state: int (default: 42)
        Random state for reordering the data
    bootstrap: bool
        Whether to bootstrap the data at each N
    bootstrap_per_N: int
        If bootstrap is True then how many bootstraps per each sample size should be performed i.e if bootstrap_per_N
        is 2 then at sample size N=20, 2 bootstraps will be performed. This will continue until starting_N == N.
    plot: bool (default: True)
        Whether to plot the results
    addition_to_title: str (default: &#39;&#39;)
        The title of the plot will be &#34;The absolute r-value between {feature1} and {feature2} as N increases&#34; and
        followed by the addition (e.g. to describe a dataset).
    alpha: float (default: 0.05)
        Threshold for p-value that should be shown in the plot
    Returns
    ----------
    cor_results: pd.DataFrame
        Dataframe with the results for all ran analyses
    fig: Figure
        Figure will be returned if plot=True, otherwise None. This allows you to change properties of the figure
        afterwards, e.g. fig.axes[0].set_title(&#39;This is my new title&#39;)
    Examples
    ----------
    &gt;&gt;&gt; import seaborn as sns
    &gt;&gt;&gt; from jmspack.frequentist_statistics import correlations_as_sample_increases
    &gt;&gt;&gt; iris = sns.load_dataset(&#39;iris&#39;)
    &gt;&gt;&gt; summary,  fig = correlations_as_sample_increases(data=iris,feature1=&#39;petal_width&#39;,feature2=&#39;sepal_length&#39;,
    &gt;&gt;&gt; starting_N=20)
    &#34;&#34;&#34;

    data = (
        data[[feature1, feature2]].copy()
        # Remove rows with np.nans
        .dropna()
        # Randomize order of the data
        .sample(frac=1, random_state=random_state)
    )
    if data.shape[0] &lt; starting_N:
        raise ValueError(&#34;Number of valid cases is smaller than the starting_N&#34;)
    if data.shape[0] &lt; starting_N + step:
        raise ValueError(
            &#34;Number of valid cases is smaller than the starting_N + step (only one correlation possible)&#34;
        )

    # Initiate data frame for results
    corr_results = pd.DataFrame()

    # Loop through all possible number of rows from starting N till number of rows
    for i in range(starting_N, data.shape[0] + 1, step):
        boot_corr_results = pd.DataFrame()
        if bootstrap:
            for boot_num in range(0, bootstrap_per_N):
                boot_data = data.sample(frac=1, random_state=boot_num)
                current_boot_corr = correlation_analysis(
                    boot_data.iloc[0:i],
                    method=method,
                    check_norm=False,
                    permutation_test=False,
                )[&#34;summary&#34;][[&#34;r-value&#34;, &#34;p-value&#34;, &#34;N&#34;]]
                boot_corr_results = pd.concat(
                    [boot_corr_results, current_boot_corr], ignore_index=True
                )
            corr_results = pd.concat(
                [corr_results, boot_corr_results], ignore_index=True
            )
        else:
            # Run correlation with all data from first row until row i
            current_corr = correlation_analysis(
                data.iloc[0:i], method=method, check_norm=False, permutation_test=False
            )[&#34;summary&#34;][[&#34;r-value&#34;, &#34;p-value&#34;, &#34;N&#34;]]
            corr_results = pd.concat([corr_results, current_corr], ignore_index=True)

    fig = None
    if plot:
        fig, ax = plt.subplots(figsize=figsize)
        # Add r-value and p-value
        _ = sns.lineplot(
            x=corr_results[&#34;N&#34;],
            y=abs(corr_results[&#34;r-value&#34;]),
            label=&#34;absolute r-value&#34;,
            ax=ax,
        ).set_title(
            f&#34;The absolute r-value between {feature1} and {feature2}\nas N increases {addition_to_title}&#34;
        )
        _ = sns.lineplot(
            x=corr_results[&#34;N&#34;], y=corr_results[&#34;p-value&#34;], label=&#34;p-value&#34;, ax=ax
        )
        # Add alpha level (threshold for p-value)
        _ = ax.axhline(
            y=alpha, color=&#34;black&#34;, alpha=0.5, linestyle=&#34;--&#34;, label=f&#34;&gt;= {alpha}&#34;
        )

        _ = ax.set_ylabel(&#34;&#34;)
        _ = ax.set_ylim(0, 1)
        _ = plt.legend()
    return corr_results, fig</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.multiple_univariate_OLSs"><code class="name flex">
<span>def <span class="ident">multiple_univariate_OLSs</span></span>(<span>X: pandas.core.frame.DataFrame, y: pandas.core.series.Series, features_list: list)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multiple_univariate_OLSs(
    X: pd.DataFrame,
    y: pd.Series,
    features_list: list,
):
    all_coefs_df = pd.DataFrame()
    for feature in features_list:
        mod = sm.OLS(endog=y, exog=sm.add_constant(X[[feature]]))
        res = mod.fit()
        coef_df = pd.read_html(
            res.summary().tables[1].as_html(), header=0, index_col=0
        )[0].drop(&#34;const&#34;)
        coef_df = coef_df.assign(
            **{&#34;rsquared&#34;: res.rsquared, &#34;rsquared_adj&#34;: res.rsquared_adj}
        )
        all_coefs_df = pd.concat([all_coefs_df, coef_df])
    return all_coefs_df</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.normal_check"><code class="name flex">
<span>def <span class="ident">normal_check</span></span>(<span>data: pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Compare the distribution of numeric variables to a normal distribution using the Kolmogrov-Smirnov test
Wrapper for <code>scipy.stats.kstest</code>: the empircal data is compared to a normally distributed variable with the
same mean and standard deviation. A significant result (p &lt; 0.05) in the goodness of fit test means that the
data is not normally distributed.
Parameters</p>
<hr>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe including the columns of interest</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_normality_check</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe with column names, p-values and an indication of normality</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">&gt;&gt;&gt; tips = sns.load_dataset(&quot;tips&quot;)
&gt;&gt;&gt; df_normality_check = normal_check(tips)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normal_check(data: pd.DataFrame) -&gt; pd.DataFrame:
    r&#34;&#34;&#34;Compare the distribution of numeric variables to a normal distribution using the Kolmogrov-Smirnov test
    Wrapper for `scipy.stats.kstest`: the empircal data is compared to a normally distributed variable with the
    same mean and standard deviation. A significant result (p &lt; 0.05) in the goodness of fit test means that the
    data is not normally distributed.
    Parameters
    ----------
    data: pandas.DataFrame
        Dataframe including the columns of interest
    Returns
    ----------
    df_normality_check: pd.DataFrame
        Dataframe with column names, p-values and an indication of normality
    Examples
    ----------
    &gt;&gt;&gt; tips = sns.load_dataset(&#34;tips&#34;)
    &gt;&gt;&gt; df_normality_check = normal_check(tips)
    &#34;&#34;&#34;
    # Select numeric columns only
    num_features = data.select_dtypes(include=&#34;number&#34;).columns.tolist()
    # Compare distribution of each feature to a normal distribution with given mean and std
    df_normality_check = data[num_features].apply(
        lambda x: stats.kstest(
            x.dropna(), stats.norm.cdf, args=(np.nanmean(x), np.nanstd(x)), N=len(x)
        )[1],
        axis=0,
    )

    # create a label that indicates whether a feature has a normal distribution or not
    df_normality_check = pd.DataFrame(df_normality_check).reset_index()
    df_normality_check.columns = [&#34;feature&#34;, &#34;p-value&#34;]
    df_normality_check[&#34;normality&#34;] = df_normality_check[&#34;p-value&#34;] &gt;= 0.05

    return df_normality_check</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.partial_correlation"><code class="name flex">
<span>def <span class="ident">partial_correlation</span></span>(<span>df: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the sample linear partial correlation coefficients between pairs of variables,
controlling for all other remaining variables
Parameters</p>
<hr>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>array-like, shape (n, p)</code></dt>
<dd>Array with the different variables. Each column is taken as a variable.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>P</code></strong> :&ensp;<code>array-like, shape (p, p)</code></dt>
<dd>P[i, j] contains the partial correlation of input_df[:, i] and input_df[:, j]
controlling for all other remaining variables.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partial_correlation(df: pd.DataFrame):
    &#34;&#34;&#34;
    Returns the sample linear partial correlation coefficients between pairs of variables,
    controlling for all other remaining variables
    Parameters
    ----------
    df : array-like, shape (n, p)
        Array with the different variables. Each column is taken as a variable.
    Returns
    -------
    P : array-like, shape (p, p)
        P[i, j] contains the partial correlation of input_df[:, i] and input_df[:, j]
        controlling for all other remaining variables.
    &#34;&#34;&#34;
    partial_corr_matrix_rvals = np.zeros((df.shape[1], df.shape[1]))
    partial_corr_matrix_pvals = np.zeros((df.shape[1], df.shape[1]))

    for i, column1 in enumerate(df):
        for j, column2 in enumerate(df):
            control_variables = np.delete(np.arange(df.shape[1]), [i, j])
            if i == j:
                partial_corr_matrix_rvals[i, j] = 1
                partial_corr_matrix_pvals[i, j] = 1
                continue
            data_control_variable = df.iloc[:, control_variables]
            data_column1 = df[column1].values
            data_column2 = df[column2].values
            fit1 = LinearRegression(fit_intercept=True)
            fit2 = LinearRegression(fit_intercept=True)
            fit1.fit(data_control_variable, data_column1)
            fit2.fit(data_control_variable, data_column2)
            residual1 = data_column1 - (
                np.dot(data_control_variable, fit1.coef_) + fit1.intercept_
            )
            residual2 = data_column2 - (
                np.dot(data_control_variable, fit2.coef_) + fit2.intercept_
            )
            partial_corr_matrix_rvals[i, j] = stats.pearsonr(residual1, residual2)[0]
            partial_corr_matrix_pvals[i, j] = stats.pearsonr(residual1, residual2)[1]
            partial_corr_matrix_rvals_df = pd.DataFrame(
                partial_corr_matrix_rvals, columns=df.columns, index=df.columns
            )
            partial_corr_matrix_pvals_df = pd.DataFrame(
                partial_corr_matrix_pvals, columns=df.columns, index=df.columns
            )

    return partial_corr_matrix_rvals_df, partial_corr_matrix_pvals_df</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.permute_test"><code class="name flex">
<span>def <span class="ident">permute_test</span></span>(<span>a, test_type, test, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to run tests for permutations
Parameters</p>
<hr>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>np.array</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>test_type</code></strong> :&ensp;<code>str {'correlation', 'independent_t_test'}</code></dt>
<dd>Type of the test to be used</dd>
</dl>
<p>test:
e.g. <code>scipy.stats.pearsonr</code> or <code>statsmodels.stats.weightstats.ttest_ind</code>
**kwargs:
Additional keywords to be added to <code>test</code>
- <code>a2</code> for the second feature if test_type = 'correlation'
Returns</p>
<hr>
<dl>
<dt><code>float:</code></dt>
<dd>p value for permutation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def permute_test(a, test_type, test, **kwargs):
    r&#34;&#34;&#34;Helper function to run tests for permutations
    Parameters
    ----------
    a : np.array
    test_type: str {&#39;correlation&#39;, &#39;independent_t_test&#39;}
        Type of the test to be used
    test:
        e.g. `scipy.stats.pearsonr` or `statsmodels.stats.weightstats.ttest_ind`
    **kwargs:
        Additional keywords to be added to `test`
        - `a2` for the second feature if test_type = &#39;correlation&#39;
    Returns
    ----------
    float:
        p value for permutation
    &#34;&#34;&#34;
    if test_type == &#34;correlation&#34;:
        a2 = kwargs[&#34;a2&#34;]
        _, p = test(a, a2)

    else:
        raise ValueError(&#34;Unknown test_type provided&#34;)</code></pre>
</details>
</dd>
<dt id="jmspack.frequentist_statistics.potential_for_change_index"><code class="name flex">
<span>def <span class="ident">potential_for_change_index</span></span>(<span>data: pandas.core.frame.DataFrame, features_list: list, target: str, minimum_measure: str = 'min', centrality_measure: str = 'mean', maximum_measure: str = 'max', weight_measure: str = 'rsquared_adj', scale_data: bool = True, pci_heatmap: bool = True, pci_heatmap_figsize: Tuple[float, float] = (1.0, 4.0))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def potential_for_change_index(
    data: pd.DataFrame,
    features_list: list,
    target: str,
    minimum_measure: str = &#34;min&#34;,
    centrality_measure: str = &#34;mean&#34;,
    maximum_measure: str = &#34;max&#34;,
    weight_measure: str = &#34;rsquared_adj&#34;,
    scale_data: bool = True,
    pci_heatmap: bool = True,
    pci_heatmap_figsize: Tuple[float, float] = (1.0, 4.0),
):
    if scale_data:
        data = data.pipe(apply_scaling)

    if weight_measure == &#34;rsquared_adj&#34; or weight_measure == &#34;rsquared&#34;:
        tmp_X = data[features_list]
        tmp_y = data[target]
        weight_df = multiple_univariate_OLSs(
            X=tmp_X, y=tmp_y, features_list=features_list
        )

        negative_list = weight_df[weight_df[&#34;coef&#34;] &lt; 0].index.tolist()

    else:
        output_dict = correlation_analysis(
            data=data,
            col_list=features_list,
            row_list=[target],
            method=&#34;pearson&#34;,
            check_norm=False,
            dropna=&#34;pairwise&#34;,
            permutation_test=False,
            n_permutations=10,
            random_state=69420,
        )
        weight_df = output_dict[&#34;summary&#34;].set_index(&#34;feature1&#34;)
        negative_list = weight_df[weight_df[&#34;r-value&#34;] &lt; 0].index.tolist()

    if len(negative_list) &lt; 0:

        pci_df = (
            # room for improvement calculation (series)
            (
                data[features_list].agg(centrality_measure)
                - (data[features_list].agg(maximum_measure))
            ).abs()
            * weight_df[weight_measure]  # weight (based on weight_measure series)
        ).to_frame(&#34;PCI&#34;)
    else:
        neg_pci_df = (
            # room for improvement calculation (series)
            (
                data[negative_list].agg(centrality_measure)
                - (data[negative_list].agg(minimum_measure))
            ).abs()
            * weight_df.loc[
                negative_list, weight_measure
            ]  # weight (based on weight_measure series)
        ).to_frame(&#34;PCI&#34;)

        pos_pci_df = (
            # room for improvement calculation (series)
            (
                data[features_list].drop(negative_list, axis=1).agg(centrality_measure)
                - (data[features_list].drop(negative_list, axis=1).agg(maximum_measure))
            ).abs()
            * weight_df[weight_measure].drop(
                negative_list, axis=0
            )  # weight (based on weight_measure series)
        ).to_frame(&#34;PCI&#34;)

        pci_df = pd.concat([pos_pci_df, neg_pci_df])

    if pci_heatmap:
        _ = plt.figure(figsize=pci_heatmap_figsize)
        _ = sns.heatmap(
            data=pci_df.sort_values(by=&#34;PCI&#34;, ascending=False),
            annot=True,
            fmt=&#34;.3g&#34;,
        )

    if weight_measure == &#34;rsquared_adj&#34; or weight_measure == &#34;rsquared&#34;:
        return pci_df.merge(
            data[features_list]
            .agg([minimum_measure, centrality_measure, maximum_measure])
            .T,
            left_index=True,
            right_index=True,
        ).merge(weight_df[[weight_measure, &#34;P&gt;|t|&#34;]], left_index=True, right_index=True)

    else:
        return pci_df.merge(
            data[features_list]
            .agg([minimum_measure, centrality_measure, maximum_measure])
            .T,
            left_index=True,
            right_index=True,
        ).merge(
            weight_df.loc[:, [weight_measure, &#34;p-value&#34;]],
            left_index=True,
            right_index=True,
        )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jmspack" href="index.html">jmspack</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="jmspack.frequentist_statistics.correct_pvalues" href="#jmspack.frequentist_statistics.correct_pvalues">correct_pvalues</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.correlation_analysis" href="#jmspack.frequentist_statistics.correlation_analysis">correlation_analysis</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.correlations_as_sample_increases" href="#jmspack.frequentist_statistics.correlations_as_sample_increases">correlations_as_sample_increases</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.multiple_univariate_OLSs" href="#jmspack.frequentist_statistics.multiple_univariate_OLSs">multiple_univariate_OLSs</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.normal_check" href="#jmspack.frequentist_statistics.normal_check">normal_check</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.partial_correlation" href="#jmspack.frequentist_statistics.partial_correlation">partial_correlation</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.permute_test" href="#jmspack.frequentist_statistics.permute_test">permute_test</a></code></li>
<li><code><a title="jmspack.frequentist_statistics.potential_for_change_index" href="#jmspack.frequentist_statistics.potential_for_change_index">potential_for_change_index</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
