<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>jmspack.NLTSA API documentation</title>
<meta name="description" content="Submodule NLTSA.py includes the following functions: &lt;br&gt;
- fluctuation_intensity(): run fluctuation intensity on a time series to detect non linear …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jmspack.NLTSA</code></h1>
</header>
<section id="section-intro">
<p>Submodule NLTSA.py includes the following functions: <br>
- fluctuation_intensity(): run fluctuation intensity on a time series to detect non linear change <br>
- distribution_uniformity(): run distribution uniformity on a time series to detect non linear change <br>
- complexity_resonance(): the product of fluctuation_intensity and distribution_uniformity <br>
- complexity_resonance_diagram(): plots a heatmap of the complexity_resonance <br>
- ts_levels(): defines distinct levels in a time series based on decision tree regressor <br>
- cmaps_options[]: a list of possible colour maps that may be used when plotting <br>
- flatten(): a utils function which flattens a list of lists into one list <br>
- cumulative_complexity_peaks(): a function which will calculate the significant peaks in the dynamic
complexity of a set of time series (these peaks are known as cumulative complexity peaks; CCPs) <br>
- cumulative_complexity_peaks_plot(): plots a heatmap of the cumulative_complexity_peaks <br></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Submodule NLTSA.py includes the following functions: &lt;br&gt;
- fluctuation_intensity(): run fluctuation intensity on a time series to detect non linear change &lt;br&gt;
- distribution_uniformity(): run distribution uniformity on a time series to detect non linear change &lt;br&gt;
- complexity_resonance(): the product of fluctuation_intensity and distribution_uniformity &lt;br&gt;
- complexity_resonance_diagram(): plots a heatmap of the complexity_resonance &lt;br&gt;
- ts_levels(): defines distinct levels in a time series based on decision tree regressor &lt;br&gt;
- cmaps_options[]: a list of possible colour maps that may be used when plotting &lt;br&gt;
- flatten(): a utils function which flattens a list of lists into one list &lt;br&gt;
- cumulative_complexity_peaks(): a function which will calculate the significant peaks in the dynamic
complexity of a set of time series (these peaks are known as cumulative complexity peaks; CCPs) &lt;br&gt;
- cumulative_complexity_peaks_plot(): plots a heatmap of the cumulative_complexity_peaks &lt;br&gt;
&#34;&#34;&#34;
import matplotlib.patheffects as pe
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.stats import norm
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor

# create flatten function to use when you have lists within lists
flatten = lambda l: [item for sublist in l for item in sublist]

cmaps_options = [
    &#34;flag&#34;,
    &#34;prism&#34;,
    &#34;ocean&#34;,
    &#34;gist_earth&#34;,
    &#34;terrain&#34;,
    &#34;gist_stern&#34;,
    &#34;gnuplot&#34;,
    &#34;gnuplot2&#34;,
    &#34;CMRmap&#34;,
    &#34;cubehelix&#34;,
    &#34;brg&#34;,
    &#34;gist_rainbow&#34;,
    &#34;rainbow&#34;,
    &#34;jet&#34;,
    &#34;nipy_spectral&#34;,
    &#34;gist_ncar&#34;,
]


def ts_levels(
    ts,
    ts_x=None,
    criterion=&#34;mse&#34;,
    max_depth=2,
    min_samples_leaf=1,
    min_samples_split=2,
    max_leaf_nodes=30,
    plot=True,
    equal_spaced=True,
    n_x_ticks=10,
    figsize=(20, 5),
):
    r&#34;&#34;&#34;Use recursive partitioning (DecisionTreeRegressor) to perform a &#39;classification&#39; of relatively stable levels in a timeseries.
    Parameters
    ---------
    ts: pandas DataFrame (column)
        A dataframe column containing a univariate time series from 1 person.
        Rows should indicate time, column should indicate the time series variable.
    ts_x: pandas DataFrame (column; Default=None)
        A dataframe column containing the corresponding timestamps to the aforementioned time series.
        If None is passed, the index of the time series will be used (Default = None).
    criterion: str (Default=&#34;mse&#34;)
        The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which
        is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each
        terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential
        splits, and “mae” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.
    max_depth: int or None, optional (default=2)
        The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves
        contain less than min_samples_split samples.
    min_samples_leaf: int, float, optional (default=1)
        The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered
        if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have
        the effect of smoothing the model, especially in regression.
        If int, then consider min_samples_leaf as the minimum number.
        If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are
        the minimum number of samples for each node.
    min_samples_split: int, float, optional (default=2)
        The minimum number of samples required to split an internal node.
        If int, then consider min_samples_split as the minimum number.
        If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum
        number of samples for each split.
    max_leaf_nodes: int or None, optional (default=30)
        Identify max_leaf_nodes amount of time series levels in the time series in best-first fashion. Best splits are
        defined as relative reduction in impurity. If None then unlimited number of splits.
    plot: boolean (Default=True)
        A boolean to define whether to plot the time series and it&#39;s time series levels.
    equal_spaced: boolean (Default=True)
        A boolean to define whether or not the time series is continuously measured or not. If False this will be taken
        into account when plotting the X-axis of the plot.
    n_x_ticks: int (Default=10)
        The amount of x-ticks you wish to show when plotting.
    figsize: tuple (Default=(20,5))
        The tuple used to specify the size of the plot if plot = True.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; ts = ts_df[&#34;lorenz&#34;]
    &gt;&gt;&gt; ts_levels_df, fig, ax = ts_levels(ts, ts_x=None, criterion=&#34;mse&#34;, max_depth=10, min_samples_leaf=1,
    &gt;&gt;&gt;                          min_samples_split=2, max_leaf_nodes=30, plot=True, equal_spaced=True, n_x_ticks=10)
    &#34;&#34;&#34;
    # Change ts to a numpy array
    if not isinstance(ts, np.ndarray):
        ts = ts.to_numpy()
    # Check whether ts has only one dimension
    if len(ts.shape) != 1:
        raise ValueError(&#34;ts is not one-dimensional&#34;)

    # Make sure the ts_x matches ts
    if ts_x is not None:
        # Change ts_x to a numpy array
        if not isinstance(ts_x, np.ndarray):
            ts_x = ts_x.to_numpy()
        # Check whether ts_x has only one dimension
        if len(ts.shape) != 1:
            raise ValueError(&#34;ts_x is not one-dimensional&#34;)
        # Check whether ts and tx_x have the same length
        if not len(ts) == len(ts_x):
            raise ValueError(&#34;ts and ts_x have different lengths&#34;)

    # predictor for the tree
    x = np.array(np.arange(len(ts)))
    x = x.reshape(-1, 1)

    dtr = DecisionTreeRegressor(
        criterion=criterion,
        max_depth=max_depth,
        min_samples_leaf=min_samples_leaf,
        min_samples_split=min_samples_split,
        max_leaf_nodes=max_leaf_nodes,
    )
    tree = dtr.fit(x, ts)
    p = tree.predict(x)

    fig = None
    ax = None

    if plot:
        # Select n indices
        idx = np.round(np.linspace(0, len(x) - 1, n_x_ticks)).astype(int)

        # X ticks and labels based on indices
        if ts_x is None:
            xticks = flatten(x[idx].tolist())
            xlabs = flatten(x[idx].tolist())
            x_plot = x

        # X labels based on ts_x
        else:
            # Plot using x indices and ts_x labels
            if equal_spaced:
                x_plot = x
                xticks = flatten(x[idx].tolist())
                xlabs = flatten(ts_x[idx].tolist())

            # Plot using ts_x indices and ts_x labels
            else:
                x_plot = ts_x
                xticks = flatten(ts_x[idx].tolist())
                xlabs = flatten(ts_x[idx].tolist())

        # _ = plt.figure(figsize=(20, 7))
        fig, ax = plt.subplots(figsize=figsize)
        _ = plt.scatter(
            x_plot,
            ts,
            s=20,
            edgecolor=&#34;#2167C5&#34;,
            c=&#34;#EB5E23&#34;,
            label=&#34;Original Time Series&#34;,
        )
        _ = plt.plot(
            x_plot,
            p,
            c=&#34;white&#34;,
            path_effects=[pe.Stroke(linewidth=5, foreground=&#34;#2167C5&#34;), pe.Normal()],
            label=&#34;Time Series Levels&#34;,
        )
        _ = plt.xticks(xticks, xlabs)
        _ = plt.ylabel(&#34;Amount&#34;)
        _ = plt.xlabel(&#34;Time&#34;)
        _ = plt.legend()
        _ = plt.title(&#34;Time Series Levels Plot&#34;)
        # _ = plt.show()

    # Store t_steps, original ts and ts_levels to a dataframe
    df_result = pd.DataFrame(
        {
            &#34;t_steps&#34;: flatten(x.tolist()),
            &#34;original_ts&#34;: ts.tolist(),
            &#34;ts_levels&#34;: p.tolist(),
        }
    )

    # Add the additional ts_x
    if ts_x is not None:
        df_result[&#34;ts_x&#34;] = ts_x

    return df_result, fig, ax


def distribution_uniformity(df, win, xmin, xmax, col_first, col_last):
    r&#34;&#34;&#34;Run distribution uniformity on a time series to detect non linear change
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the time series variables.
        All time series in df should be on the same scale. Otherwise the comparisons across
        time series will make no sense.
    win: int
        Size of sliding window in which to calculate distribution uniformity
        (amount of data considered in each evaluation of change).
    xmin: int
        The theoretical minimum that the values in the time series can take (scaling?)
    xmax: int
        The theoretical maximum that the values in the time series can take (scaling?)
    col_first: int
        The first column index you wish to be included in the calculation (index starts at 1!)
    col_last: int
        The last column index you wish to be included in the calculation (index starts at 1!)

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &#34;&#34;&#34;

    col_first = int(col_first)
    col_last = int(col_last)
    win = int(win)
    xmin = int(xmin)
    xmax = int(xmax)

    nrow = len(df) + 2
    ncol = col_last - col_first + 1

    ew_data_D = np.zeros((nrow, ncol))
    ew_data_D = pd.DataFrame(ew_data_D)

    for column in range(col_first - 1, col_last):
        ts = df.iloc[:, column : column + 1].values
        # s = xmax - xmin
        y = np.linspace(xmin, xmax, win)

        for i in range(0, len(ts) - win + 1):
            x = ts[i : i + win]
            x = np.sort(x, axis=None)
            r = 0
            g = 0

            for e in range(0, win - 1):
                for d in range(e + 1, win):
                    for a in range(e, d):
                        for b in range(a + 1, d + 1):
                            h = np.heaviside((y[b] - y[a]) - (x[b] - x[a]), 0)
                            if h == 1:
                                r += (y[b] - y[a]) - (x[b] - x[a])
                                g += y[b] - y[a]

            ew_data_D.iloc[i + win - 1, column - col_first + 1] = 1.0 - r / g

        distribution_uniformity_df = pd.DataFrame(ew_data_D.iloc[0 : len(df), :])

        distribution_uniformity_df.columns = df.columns.tolist()
        distribution_uniformity_df.index = df.index.tolist()

    return distribution_uniformity_df


def fluctuation_intensity(df, win, xmin, xmax, col_first, col_last):
    r&#34;&#34;&#34;Run fluctuation intensity on a time series to detect non linear change
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the time series variables.
        All time series in df should be on the same scale. Otherwise the comparisons across
        time series will make no sense.
    win: int
        Size of sliding window in which to calculate fluctuation intensity
        (amount of data considered in each evaluation of change).
    xmin: int
        The theoretical minimum that the values in the time series can take (scaling?)
    xmax: int
        The theoretical maximum that the values in the time series can take (scaling?)
    col_first: int
        The first column index you wish to be included in the calculation (index starts at 1!)
    col_last: int
        The last column index you wish to be included in the calculation (index starts at 1!)

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &#34;&#34;&#34;

    col_first = int(col_first)
    col_last = int(col_last)
    win = int(win)
    xmin = int(xmin)
    xmax = int(xmax)

    nrow = len(df)
    ncol = col_last - col_first + 1

    ew_data_F = np.zeros((nrow, ncol))
    ew_data_F = pd.DataFrame(ew_data_F)

    newrows = df.iloc[0:1, :].copy()
    newrows.iloc[:, col_first - 1 : col_last] = 0.0

    data = df.append(newrows)
    data = data.append(newrows)
    data.reset_index(inplace=True, drop=True)

    s = xmax - xmin
    length_ts = len(data)

    for column in range(col_first - 1, col_last):
        distance = 1
        ts = data.iloc[:, column : column + 1].values

        for i in range(0, length_ts - win - 1):
            y = [0] * (win - 1)
            fluct = [0] * (win - 1)
            k = [0] * (win - 1)
            dist_next = 1

            for j in range(0, win - 1):
                if (ts[i + j + 1] &gt;= ts[i + j]) and (ts[i + j + 1] &gt; ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] &lt;= ts[i + j]) and (ts[i + j + 1] &lt; ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] &gt; ts[i + j]) and (ts[i + j + 1] == ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] &lt; ts[i + j]) and (ts[i + j + 1] == ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] == ts[i + j]) and (ts[i + j + 1] &gt; ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] == ts[i + j]) and (ts[i + j + 1] &lt; ts[i + j + 2]):
                    k[j] = 1
                else:
                    k[j] = 0

            k[win - 2] = 1

            for g in range(0, len(k)):
                if k[g] == 1:
                    y[g] = abs(ts[i + g + 1] - ts[i + g + 1 - dist_next])
                    fluct[g] = y[g] / ((i + g + 2) - (i + g + 2 - dist_next))
                    dist_next = distance
                else:
                    y[g] = 0
                    fluct[g] = 0
                    dist_next += 1

            summation = 0.0
            for num in fluct:
                summation = summation + num / (s * (win - 1))

            ew_data_F.iloc[i + win - 1, column - col_first + 1] = summation

        fluctuation_intensity_df = pd.DataFrame(ew_data_F)

        fluctuation_intensity_df.columns = df.columns.tolist()
        fluctuation_intensity_df.index = df.index.tolist()

    return fluctuation_intensity_df


def complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df):
    r&#34;&#34;&#34;Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
    Parameters
    ---------
    distribution_uniformity_df: pandas DataFrame
        A dataframe containing distribution uniformity values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the distribution uniformity.
    fluctuation_intensity_df: pandas DataFrame
        A dataframe containing fluctuation intensity values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the fluctuation intensity.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &#34;&#34;&#34;
    complexity_resonance_df = distribution_uniformity_df * fluctuation_intensity_df
    return complexity_resonance_df


def complexity_resonance_diagram(
    df,
    cmap_n: int = 12,
    plot_title=&#34;Complexity Resonance Diagram&#34;,
    labels_n=10,
    figsize=(20, 7),
):
    r&#34;&#34;&#34;Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing complexity resonance values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the complexity resonance.
    cmap_n: int (Default=12)
        An integer indicating which colour map to use when plotting the heatmap. These values correspond to the index
        of the cmaps_options list ([&#39;flag&#39;, &#39;prism&#39;, &#39;ocean&#39;, &#39;gist_earth&#39;, &#39;terrain&#39;, &#39;gist_stern&#39;,
        &#39;gnuplot&#39;, &#39;gnuplot2&#39;, &#39;CMRmap&#39;, &#39;cubehelix&#39;, &#39;brg&#39;, &#39;gist_rainbow&#39;, &#39;rainbow&#39;, &#39;jet&#39;, &#39;nipy_spectral&#39;,
        &#39;gist_ncar&#39;]). Index=12 corresponds to &#39;rainbow.
    plot_title: str
        A string indicating the title to be used at the top of the plot
    labels_n: int (Default=10)
        An integer indicating the nth value to be taken for the x-axis of the plot. So if the x-axis consists of
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and the labels_n value is set to 2, then 2, 4, 6, 8, 10, will be shown on the
        x-axis of the plot.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &gt;&gt;&gt; complexity_resonance_diagram(complexity_resonance_df, cmap_n=12, labels_n=30)
    &#34;&#34;&#34;

    df_for_plot = df.copy()
    # df_for_plot.insert(loc=0, column=&#34;&#34;, value=np.nan)

    # plot the complexity resonance diagram
    fig, ax = plt.subplots(figsize=figsize)

    plot_comp = plt.imshow(df_for_plot.T, cmap=cmaps_options[cmap_n])
    plt.gca().set_aspect(aspect=&#34;auto&#34;)

    # set the color bar on the right, based on the values from the data
    _ = fig.colorbar(plot_comp)

    # Show all ticks
    ax.set_xticks(np.arange(0, len(df_for_plot)))
    ax.set_yticks(np.arange(0, len(list(df_for_plot))))

    # and label them with the respective list entries
    ax.set_xticklabels(list(df_for_plot.index))
    ax.set_yticklabels(list(df_for_plot))
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=60, ha=&#34;right&#34;, rotation_mode=&#34;anchor&#34;)

    # select the amount of labels on the X-axis you want visible (default=10)
    # Keeps every nth label
    [
        l.set_visible(False)
        for (i, l) in enumerate(ax.xaxis.get_ticklabels())
        if i % labels_n != 0
    ]

    # set the axis title
    ax.set_title(plot_title)
    # plt.show()

    return ax


def cumulative_complexity_peaks(
    df: pd.DataFrame,
    significant_level_item: float = 0.05,
    significant_level_time: float = 0.05,
):
    r&#34;&#34;&#34;Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing complexity resonance values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the complexity resonance.
    significant_level_item: float (Default=0.05)
        A float indicating the cutoff of when a point in time is significantly different than the rest on an individual item level (i.e.
        is this time point different than all the other time points for this item/ feature).
    significant_level_time: float (Default=0.05)
        A float indicating the cutoff of when a point in time is significantly different than the rest on a timepoint level
        (i.e. is this day different than all the other days).

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &gt;&gt;&gt; cumulative_complexity_peaks_df, significant_peaks_df = cumulative_complexity_peaks(df=complexity_resonance_df)
    &#34;&#34;&#34;

    ## Creating CCP data frame
    z_scale_cr_df = pd.DataFrame(
        StandardScaler().fit_transform(df), columns=df.columns.tolist()
    )
    ccp_df = z_scale_cr_df.mask(
        z_scale_cr_df &gt; norm.ppf(1 - significant_level_item), 1
    ).mask(z_scale_cr_df &lt;= norm.ppf(1 - significant_level_item), 0)

    ## Creating significant CCP time points data frame (1 column)
    z_scale_sum_ccp_df = pd.DataFrame(
        StandardScaler().fit_transform(ccp_df.sum(axis=1).values.reshape(-1, 1)),
        columns=[&#34;Significant CCPs&#34;],
    )
    sig_peaks_df = z_scale_sum_ccp_df.mask(
        z_scale_sum_ccp_df &gt; norm.ppf(1 - significant_level_time), 1
    ).mask(z_scale_sum_ccp_df &lt;= norm.ppf(1 - significant_level_time), 0)

    ccp_df.columns = df.columns.tolist()
    ccp_df.index = df.index.tolist()

    sig_peaks_df.index = df.index.tolist()

    return ccp_df, sig_peaks_df


def cumulative_complexity_peaks_plot(
    cumulative_complexity_peaks_df: pd.DataFrame,
    significant_peaks_df: pd.DataFrame,
    plot_title: str = &#34;Cumulative Complexity Peaks Plot&#34;,
    figsize: tuple = (20, 5),
    height_ratios: list = [1, 3],
    labels_n: int = 10,
):
    r&#34;&#34;&#34;Create a cumulative complexity peaks plot based on the cumulative_complexity_peaks_df and the significant_peaks_df
    Parameters
    ---------
    cumulative_complexity_peaks_df: pandas DataFrame
        A dataframe containing cumulative complexity peaks values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the cumulative complexity peaks.
    significant_peaks_df: pandas DataFrame
        A dataframe containing one column of significant complexity peaks values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the significant cumulative complexity peaks.
    plot_title: str
        A string indicating the title to be used at the top of the plot
    figsize: tuple (Default=(20,5))
        The tuple used to specify the size of the plot.
    height_ratios: list (Default=[1,3])
        The tuple used to specify the size of the plot.
    labels_n: int (Default=10)
        An integer indicating the nth value to be taken for the x-axis of the plot. So if the x-axis consists of
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and the labels_n value is set to 2, then 2, 4, 6, 8, 10, will be shown on the
        x-axis of the plot.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;datasets/time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &gt;&gt;&gt; cumulative_complexity_peaks_df, significant_peaks_df = cumulative_complexity_peaks(df=complexity_resonance_df)
    &gt;&gt;&gt; _ = cumulative_complexity_peaks_plot(cumulative_complexity_peaks_df=cumulative_complexity_peaks_df, significant_peaks_df=significant_peaks_df)
    &#34;&#34;&#34;
    custom_cmap = sns.color_palette([&#34;#FFFFFF&#34;, &#34;#000000&#34;])

    fig, axes = plt.subplots(
        2, 1, figsize=figsize, gridspec_kw={&#34;height_ratios&#34;: height_ratios}
    )

    axe = sns.heatmap(significant_peaks_df.T, cmap=custom_cmap, cbar=False, ax=axes[0])
    _ = axe.set_xticks([])
    _ = axe.get_yticklabels()[0].set_rotation(0)

    _ = axe.set_title(plot_title)

    ax = sns.heatmap(
        cumulative_complexity_peaks_df.T, cmap=custom_cmap, cbar=False, ax=axes[1]
    )

    # Show all ticks
    ax.set_xticks(np.arange(0, len(cumulative_complexity_peaks_df)))
    ax.set_yticks(np.arange(0, len(list(cumulative_complexity_peaks_df))))

    # and label them with the respective list entries
    ax.set_xticklabels(list(cumulative_complexity_peaks_df.index))
    ax.set_yticklabels(list(cumulative_complexity_peaks_df))
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=60, ha=&#34;right&#34;, rotation_mode=&#34;anchor&#34;)

    # select the amount of labels on the X-axis you want visible (default=10)
    # Keeps every nth label
    [
        l.set_visible(False)
        for (i, l) in enumerate(ax.xaxis.get_ticklabels())
        if i % labels_n != 0
    ]

    return fig, axes</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="jmspack.NLTSA.complexity_resonance"><code class="name flex">
<span>def <span class="ident">complexity_resonance</span></span>(<span>distribution_uniformity_df, fluctuation_intensity_df)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
Parameters</p>
<hr>
<dl>
<dt><strong><code>distribution_uniformity_df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing distribution uniformity values from multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the distribution uniformity.</dd>
<dt><strong><code>fluctuation_intensity_df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing fluctuation intensity values from multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the fluctuation intensity.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; scaler = MinMaxScaler()
&gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
&gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df):
    r&#34;&#34;&#34;Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
    Parameters
    ---------
    distribution_uniformity_df: pandas DataFrame
        A dataframe containing distribution uniformity values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the distribution uniformity.
    fluctuation_intensity_df: pandas DataFrame
        A dataframe containing fluctuation intensity values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the fluctuation intensity.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &#34;&#34;&#34;
    complexity_resonance_df = distribution_uniformity_df * fluctuation_intensity_df
    return complexity_resonance_df</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.complexity_resonance_diagram"><code class="name flex">
<span>def <span class="ident">complexity_resonance_diagram</span></span>(<span>df, cmap_n: int = 12, plot_title='Complexity Resonance Diagram', labels_n=10, figsize=(20, 7))</span>
</code></dt>
<dd>
<div class="desc"><p>Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
Parameters</p>
<hr>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing complexity resonance values from multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the complexity resonance.</dd>
<dt><strong><code>cmap_n</code></strong> :&ensp;<code>int (Default=12)</code></dt>
<dd>An integer indicating which colour map to use when plotting the heatmap. These values correspond to the index
of the cmaps_options list (['flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',
'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg', 'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral',
'gist_ncar']). Index=12 corresponds to 'rainbow.</dd>
<dt><strong><code>plot_title</code></strong> :&ensp;<code>str</code></dt>
<dd>A string indicating the title to be used at the top of the plot</dd>
<dt><strong><code>labels_n</code></strong> :&ensp;<code>int (Default=10)</code></dt>
<dd>An integer indicating the nth value to be taken for the x-axis of the plot. So if the x-axis consists of
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and the labels_n value is set to 2, then 2, 4, 6, 8, 10, will be shown on the
x-axis of the plot.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; scaler = MinMaxScaler()
&gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
&gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
&gt;&gt;&gt; complexity_resonance_diagram(complexity_resonance_df, cmap_n=12, labels_n=30)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def complexity_resonance_diagram(
    df,
    cmap_n: int = 12,
    plot_title=&#34;Complexity Resonance Diagram&#34;,
    labels_n=10,
    figsize=(20, 7),
):
    r&#34;&#34;&#34;Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing complexity resonance values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the complexity resonance.
    cmap_n: int (Default=12)
        An integer indicating which colour map to use when plotting the heatmap. These values correspond to the index
        of the cmaps_options list ([&#39;flag&#39;, &#39;prism&#39;, &#39;ocean&#39;, &#39;gist_earth&#39;, &#39;terrain&#39;, &#39;gist_stern&#39;,
        &#39;gnuplot&#39;, &#39;gnuplot2&#39;, &#39;CMRmap&#39;, &#39;cubehelix&#39;, &#39;brg&#39;, &#39;gist_rainbow&#39;, &#39;rainbow&#39;, &#39;jet&#39;, &#39;nipy_spectral&#39;,
        &#39;gist_ncar&#39;]). Index=12 corresponds to &#39;rainbow.
    plot_title: str
        A string indicating the title to be used at the top of the plot
    labels_n: int (Default=10)
        An integer indicating the nth value to be taken for the x-axis of the plot. So if the x-axis consists of
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and the labels_n value is set to 2, then 2, 4, 6, 8, 10, will be shown on the
        x-axis of the plot.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &gt;&gt;&gt; complexity_resonance_diagram(complexity_resonance_df, cmap_n=12, labels_n=30)
    &#34;&#34;&#34;

    df_for_plot = df.copy()
    # df_for_plot.insert(loc=0, column=&#34;&#34;, value=np.nan)

    # plot the complexity resonance diagram
    fig, ax = plt.subplots(figsize=figsize)

    plot_comp = plt.imshow(df_for_plot.T, cmap=cmaps_options[cmap_n])
    plt.gca().set_aspect(aspect=&#34;auto&#34;)

    # set the color bar on the right, based on the values from the data
    _ = fig.colorbar(plot_comp)

    # Show all ticks
    ax.set_xticks(np.arange(0, len(df_for_plot)))
    ax.set_yticks(np.arange(0, len(list(df_for_plot))))

    # and label them with the respective list entries
    ax.set_xticklabels(list(df_for_plot.index))
    ax.set_yticklabels(list(df_for_plot))
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=60, ha=&#34;right&#34;, rotation_mode=&#34;anchor&#34;)

    # select the amount of labels on the X-axis you want visible (default=10)
    # Keeps every nth label
    [
        l.set_visible(False)
        for (i, l) in enumerate(ax.xaxis.get_ticklabels())
        if i % labels_n != 0
    ]

    # set the axis title
    ax.set_title(plot_title)
    # plt.show()

    return ax</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.cumulative_complexity_peaks"><code class="name flex">
<span>def <span class="ident">cumulative_complexity_peaks</span></span>(<span>df: pandas.core.frame.DataFrame, significant_level_item: float = 0.05, significant_level_time: float = 0.05)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
Parameters</p>
<hr>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing complexity resonance values from multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the complexity resonance.</dd>
<dt><strong><code>significant_level_item</code></strong> :&ensp;<code>float (Default=0.05)</code></dt>
<dd>A float indicating the cutoff of when a point in time is significantly different than the rest on an individual item level (i.e.
is this time point different than all the other time points for this item/ feature).</dd>
<dt><strong><code>significant_level_time</code></strong> :&ensp;<code>float (Default=0.05)</code></dt>
<dd>A float indicating the cutoff of when a point in time is significantly different than the rest on a timepoint level
(i.e. is this day different than all the other days).</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; scaler = MinMaxScaler()
&gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
&gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
&gt;&gt;&gt; cumulative_complexity_peaks_df, significant_peaks_df = cumulative_complexity_peaks(df=complexity_resonance_df)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cumulative_complexity_peaks(
    df: pd.DataFrame,
    significant_level_item: float = 0.05,
    significant_level_time: float = 0.05,
):
    r&#34;&#34;&#34;Create a complexity resonance data frame based on the product of the distribution uniformity and the fluctuation intensity
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing complexity resonance values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the complexity resonance.
    significant_level_item: float (Default=0.05)
        A float indicating the cutoff of when a point in time is significantly different than the rest on an individual item level (i.e.
        is this time point different than all the other time points for this item/ feature).
    significant_level_time: float (Default=0.05)
        A float indicating the cutoff of when a point in time is significantly different than the rest on a timepoint level
        (i.e. is this day different than all the other days).

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &gt;&gt;&gt; cumulative_complexity_peaks_df, significant_peaks_df = cumulative_complexity_peaks(df=complexity_resonance_df)
    &#34;&#34;&#34;

    ## Creating CCP data frame
    z_scale_cr_df = pd.DataFrame(
        StandardScaler().fit_transform(df), columns=df.columns.tolist()
    )
    ccp_df = z_scale_cr_df.mask(
        z_scale_cr_df &gt; norm.ppf(1 - significant_level_item), 1
    ).mask(z_scale_cr_df &lt;= norm.ppf(1 - significant_level_item), 0)

    ## Creating significant CCP time points data frame (1 column)
    z_scale_sum_ccp_df = pd.DataFrame(
        StandardScaler().fit_transform(ccp_df.sum(axis=1).values.reshape(-1, 1)),
        columns=[&#34;Significant CCPs&#34;],
    )
    sig_peaks_df = z_scale_sum_ccp_df.mask(
        z_scale_sum_ccp_df &gt; norm.ppf(1 - significant_level_time), 1
    ).mask(z_scale_sum_ccp_df &lt;= norm.ppf(1 - significant_level_time), 0)

    ccp_df.columns = df.columns.tolist()
    ccp_df.index = df.index.tolist()

    sig_peaks_df.index = df.index.tolist()

    return ccp_df, sig_peaks_df</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.cumulative_complexity_peaks_plot"><code class="name flex">
<span>def <span class="ident">cumulative_complexity_peaks_plot</span></span>(<span>cumulative_complexity_peaks_df: pandas.core.frame.DataFrame, significant_peaks_df: pandas.core.frame.DataFrame, plot_title: str = 'Cumulative Complexity Peaks Plot', figsize: tuple = (20, 5), height_ratios: list = [1, 3], labels_n: int = 10)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a cumulative complexity peaks plot based on the cumulative_complexity_peaks_df and the significant_peaks_df
Parameters</p>
<hr>
<dl>
<dt><strong><code>cumulative_complexity_peaks_df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing cumulative complexity peaks values from multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the cumulative complexity peaks.</dd>
<dt><strong><code>significant_peaks_df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing one column of significant complexity peaks values from multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the significant cumulative complexity peaks.</dd>
<dt><strong><code>plot_title</code></strong> :&ensp;<code>str</code></dt>
<dd>A string indicating the title to be used at the top of the plot</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple (Default=(20,5))</code></dt>
<dd>The tuple used to specify the size of the plot.</dd>
<dt><strong><code>height_ratios</code></strong> :&ensp;<code>list (Default=[1,3])</code></dt>
<dd>The tuple used to specify the size of the plot.</dd>
<dt><strong><code>labels_n</code></strong> :&ensp;<code>int (Default=10)</code></dt>
<dd>An integer indicating the nth value to be taken for the x-axis of the plot. So if the x-axis consists of
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and the labels_n value is set to 2, then 2, 4, 6, 8, 10, will be shown on the
x-axis of the plot.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;datasets/time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; scaler = MinMaxScaler()
&gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
&gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
&gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
&gt;&gt;&gt; cumulative_complexity_peaks_df, significant_peaks_df = cumulative_complexity_peaks(df=complexity_resonance_df)
&gt;&gt;&gt; _ = cumulative_complexity_peaks_plot(cumulative_complexity_peaks_df=cumulative_complexity_peaks_df, significant_peaks_df=significant_peaks_df)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cumulative_complexity_peaks_plot(
    cumulative_complexity_peaks_df: pd.DataFrame,
    significant_peaks_df: pd.DataFrame,
    plot_title: str = &#34;Cumulative Complexity Peaks Plot&#34;,
    figsize: tuple = (20, 5),
    height_ratios: list = [1, 3],
    labels_n: int = 10,
):
    r&#34;&#34;&#34;Create a cumulative complexity peaks plot based on the cumulative_complexity_peaks_df and the significant_peaks_df
    Parameters
    ---------
    cumulative_complexity_peaks_df: pandas DataFrame
        A dataframe containing cumulative complexity peaks values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the cumulative complexity peaks.
    significant_peaks_df: pandas DataFrame
        A dataframe containing one column of significant complexity peaks values from multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the significant cumulative complexity peaks.
    plot_title: str
        A string indicating the title to be used at the top of the plot
    figsize: tuple (Default=(20,5))
        The tuple used to specify the size of the plot.
    height_ratios: list (Default=[1,3])
        The tuple used to specify the size of the plot.
    labels_n: int (Default=10)
        An integer indicating the nth value to be taken for the x-axis of the plot. So if the x-axis consists of
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and the labels_n value is set to 2, then 2, 4, 6, 8, 10, will be shown on the
        x-axis of the plot.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;datasets/time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &gt;&gt;&gt; complexity_resonance_df = complexity_resonance(distribution_uniformity_df, fluctuation_intensity_df)
    &gt;&gt;&gt; cumulative_complexity_peaks_df, significant_peaks_df = cumulative_complexity_peaks(df=complexity_resonance_df)
    &gt;&gt;&gt; _ = cumulative_complexity_peaks_plot(cumulative_complexity_peaks_df=cumulative_complexity_peaks_df, significant_peaks_df=significant_peaks_df)
    &#34;&#34;&#34;
    custom_cmap = sns.color_palette([&#34;#FFFFFF&#34;, &#34;#000000&#34;])

    fig, axes = plt.subplots(
        2, 1, figsize=figsize, gridspec_kw={&#34;height_ratios&#34;: height_ratios}
    )

    axe = sns.heatmap(significant_peaks_df.T, cmap=custom_cmap, cbar=False, ax=axes[0])
    _ = axe.set_xticks([])
    _ = axe.get_yticklabels()[0].set_rotation(0)

    _ = axe.set_title(plot_title)

    ax = sns.heatmap(
        cumulative_complexity_peaks_df.T, cmap=custom_cmap, cbar=False, ax=axes[1]
    )

    # Show all ticks
    ax.set_xticks(np.arange(0, len(cumulative_complexity_peaks_df)))
    ax.set_yticks(np.arange(0, len(list(cumulative_complexity_peaks_df))))

    # and label them with the respective list entries
    ax.set_xticklabels(list(cumulative_complexity_peaks_df.index))
    ax.set_yticklabels(list(cumulative_complexity_peaks_df))
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=60, ha=&#34;right&#34;, rotation_mode=&#34;anchor&#34;)

    # select the amount of labels on the X-axis you want visible (default=10)
    # Keeps every nth label
    [
        l.set_visible(False)
        for (i, l) in enumerate(ax.xaxis.get_ticklabels())
        if i % labels_n != 0
    ]

    return fig, axes</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.distribution_uniformity"><code class="name flex">
<span>def <span class="ident">distribution_uniformity</span></span>(<span>df, win, xmin, xmax, col_first, col_last)</span>
</code></dt>
<dd>
<div class="desc"><p>Run distribution uniformity on a time series to detect non linear change
Parameters</p>
<hr>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the time series variables.
All time series in df should be on the same scale. Otherwise the comparisons across
time series will make no sense.</dd>
<dt><strong><code>win</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of sliding window in which to calculate distribution uniformity
(amount of data considered in each evaluation of change).</dd>
<dt><strong><code>xmin</code></strong> :&ensp;<code>int</code></dt>
<dd>The theoretical minimum that the values in the time series can take (scaling?)</dd>
<dt><strong><code>xmax</code></strong> :&ensp;<code>int</code></dt>
<dd>The theoretical maximum that the values in the time series can take (scaling?)</dd>
<dt><strong><code>col_first</code></strong> :&ensp;<code>int</code></dt>
<dd>The first column index you wish to be included in the calculation (index starts at 1!)</dd>
<dt><strong><code>col_last</code></strong> :&ensp;<code>int</code></dt>
<dd>The last column index you wish to be included in the calculation (index starts at 1!)</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; scaler = MinMaxScaler()
&gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
&gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distribution_uniformity(df, win, xmin, xmax, col_first, col_last):
    r&#34;&#34;&#34;Run distribution uniformity on a time series to detect non linear change
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the time series variables.
        All time series in df should be on the same scale. Otherwise the comparisons across
        time series will make no sense.
    win: int
        Size of sliding window in which to calculate distribution uniformity
        (amount of data considered in each evaluation of change).
    xmin: int
        The theoretical minimum that the values in the time series can take (scaling?)
    xmax: int
        The theoretical maximum that the values in the time series can take (scaling?)
    col_first: int
        The first column index you wish to be included in the calculation (index starts at 1!)
    col_last: int
        The last column index you wish to be included in the calculation (index starts at 1!)

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; distribution_uniformity_df = pd.DataFrame(distribution_uniformity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; distribution_uniformity_df.columns=scaled_ts_df.columns.tolist()
    &#34;&#34;&#34;

    col_first = int(col_first)
    col_last = int(col_last)
    win = int(win)
    xmin = int(xmin)
    xmax = int(xmax)

    nrow = len(df) + 2
    ncol = col_last - col_first + 1

    ew_data_D = np.zeros((nrow, ncol))
    ew_data_D = pd.DataFrame(ew_data_D)

    for column in range(col_first - 1, col_last):
        ts = df.iloc[:, column : column + 1].values
        # s = xmax - xmin
        y = np.linspace(xmin, xmax, win)

        for i in range(0, len(ts) - win + 1):
            x = ts[i : i + win]
            x = np.sort(x, axis=None)
            r = 0
            g = 0

            for e in range(0, win - 1):
                for d in range(e + 1, win):
                    for a in range(e, d):
                        for b in range(a + 1, d + 1):
                            h = np.heaviside((y[b] - y[a]) - (x[b] - x[a]), 0)
                            if h == 1:
                                r += (y[b] - y[a]) - (x[b] - x[a])
                                g += y[b] - y[a]

            ew_data_D.iloc[i + win - 1, column - col_first + 1] = 1.0 - r / g

        distribution_uniformity_df = pd.DataFrame(ew_data_D.iloc[0 : len(df), :])

        distribution_uniformity_df.columns = df.columns.tolist()
        distribution_uniformity_df.index = df.index.tolist()

    return distribution_uniformity_df</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>l)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">flatten = lambda l: [item for sublist in l for item in sublist]</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.fluctuation_intensity"><code class="name flex">
<span>def <span class="ident">fluctuation_intensity</span></span>(<span>df, win, xmin, xmax, col_first, col_last)</span>
</code></dt>
<dd>
<div class="desc"><p>Run fluctuation intensity on a time series to detect non linear change
Parameters</p>
<hr>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>A dataframe containing multivariate time series data from 1 person.
Rows should indicate time, columns should indicate the time series variables.
All time series in df should be on the same scale. Otherwise the comparisons across
time series will make no sense.</dd>
<dt><strong><code>win</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of sliding window in which to calculate fluctuation intensity
(amount of data considered in each evaluation of change).</dd>
<dt><strong><code>xmin</code></strong> :&ensp;<code>int</code></dt>
<dd>The theoretical minimum that the values in the time series can take (scaling?)</dd>
<dt><strong><code>xmax</code></strong> :&ensp;<code>int</code></dt>
<dd>The theoretical maximum that the values in the time series can take (scaling?)</dd>
<dt><strong><code>col_first</code></strong> :&ensp;<code>int</code></dt>
<dd>The first column index you wish to be included in the calculation (index starts at 1!)</dd>
<dt><strong><code>col_last</code></strong> :&ensp;<code>int</code></dt>
<dd>The last column index you wish to be included in the calculation (index starts at 1!)</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; scaler = MinMaxScaler()
&gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
&gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
&gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fluctuation_intensity(df, win, xmin, xmax, col_first, col_last):
    r&#34;&#34;&#34;Run fluctuation intensity on a time series to detect non linear change
    Parameters
    ---------
    df: pandas DataFrame
        A dataframe containing multivariate time series data from 1 person.
        Rows should indicate time, columns should indicate the time series variables.
        All time series in df should be on the same scale. Otherwise the comparisons across
        time series will make no sense.
    win: int
        Size of sliding window in which to calculate fluctuation intensity
        (amount of data considered in each evaluation of change).
    xmin: int
        The theoretical minimum that the values in the time series can take (scaling?)
    xmax: int
        The theoretical maximum that the values in the time series can take (scaling?)
    col_first: int
        The first column index you wish to be included in the calculation (index starts at 1!)
    col_last: int
        The last column index you wish to be included in the calculation (index starts at 1!)

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; scaler = MinMaxScaler()
    &gt;&gt;&gt; scaled_ts_df = pd.DataFrame(scaler.fit_transform(ts_df), columns=ts_df.columns.tolist())
    &gt;&gt;&gt; fluctuation_intensity_df = pd.DataFrame(fluctuation_intensity(scaled_ts_df, win=7, xmin=0, xmax=1, col_first=1, col_last=7))
    &gt;&gt;&gt; fluctuation_intensity_df.columns=scaled_ts_df.columns.tolist()
    &#34;&#34;&#34;

    col_first = int(col_first)
    col_last = int(col_last)
    win = int(win)
    xmin = int(xmin)
    xmax = int(xmax)

    nrow = len(df)
    ncol = col_last - col_first + 1

    ew_data_F = np.zeros((nrow, ncol))
    ew_data_F = pd.DataFrame(ew_data_F)

    newrows = df.iloc[0:1, :].copy()
    newrows.iloc[:, col_first - 1 : col_last] = 0.0

    data = df.append(newrows)
    data = data.append(newrows)
    data.reset_index(inplace=True, drop=True)

    s = xmax - xmin
    length_ts = len(data)

    for column in range(col_first - 1, col_last):
        distance = 1
        ts = data.iloc[:, column : column + 1].values

        for i in range(0, length_ts - win - 1):
            y = [0] * (win - 1)
            fluct = [0] * (win - 1)
            k = [0] * (win - 1)
            dist_next = 1

            for j in range(0, win - 1):
                if (ts[i + j + 1] &gt;= ts[i + j]) and (ts[i + j + 1] &gt; ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] &lt;= ts[i + j]) and (ts[i + j + 1] &lt; ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] &gt; ts[i + j]) and (ts[i + j + 1] == ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] &lt; ts[i + j]) and (ts[i + j + 1] == ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] == ts[i + j]) and (ts[i + j + 1] &gt; ts[i + j + 2]):
                    k[j] = 1
                elif (ts[i + j + 1] == ts[i + j]) and (ts[i + j + 1] &lt; ts[i + j + 2]):
                    k[j] = 1
                else:
                    k[j] = 0

            k[win - 2] = 1

            for g in range(0, len(k)):
                if k[g] == 1:
                    y[g] = abs(ts[i + g + 1] - ts[i + g + 1 - dist_next])
                    fluct[g] = y[g] / ((i + g + 2) - (i + g + 2 - dist_next))
                    dist_next = distance
                else:
                    y[g] = 0
                    fluct[g] = 0
                    dist_next += 1

            summation = 0.0
            for num in fluct:
                summation = summation + num / (s * (win - 1))

            ew_data_F.iloc[i + win - 1, column - col_first + 1] = summation

        fluctuation_intensity_df = pd.DataFrame(ew_data_F)

        fluctuation_intensity_df.columns = df.columns.tolist()
        fluctuation_intensity_df.index = df.index.tolist()

    return fluctuation_intensity_df</code></pre>
</details>
</dd>
<dt id="jmspack.NLTSA.ts_levels"><code class="name flex">
<span>def <span class="ident">ts_levels</span></span>(<span>ts, ts_x=None, criterion='mse', max_depth=2, min_samples_leaf=1, min_samples_split=2, max_leaf_nodes=30, plot=True, equal_spaced=True, n_x_ticks=10, figsize=(20, 5))</span>
</code></dt>
<dd>
<div class="desc"><p>Use recursive partitioning (DecisionTreeRegressor) to perform a 'classification' of relatively stable levels in a timeseries.
Parameters</p>
<hr>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas DataFrame (column)</code></dt>
<dd>A dataframe column containing a univariate time series from 1 person.
Rows should indicate time, column should indicate the time series variable.</dd>
<dt><strong><code>ts_x</code></strong> :&ensp;<code>pandas DataFrame (column; Default=None)</code></dt>
<dd>A dataframe column containing the corresponding timestamps to the aforementioned time series.
If None is passed, the index of the time series will be used (Default = None).</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>str (Default="mse")</code></dt>
<dd>The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which
is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each
terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential
splits, and “mae” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</dd>
<dt><strong><code>max_depth</code></strong> :&ensp;<code>int</code> or <code>None</code>, optional <code>(default=2)</code></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves
contain less than min_samples_split samples.</dd>
<dt><strong><code>min_samples_leaf</code></strong> :&ensp;<code>int, float</code>, optional <code>(default=1)</code></dt>
<dd>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered
if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have
the effect of smoothing the model, especially in regression.
If int, then consider min_samples_leaf as the minimum number.
If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are
the minimum number of samples for each node.</dd>
<dt><strong><code>min_samples_split</code></strong> :&ensp;<code>int, float</code>, optional <code>(default=2)</code></dt>
<dd>The minimum number of samples required to split an internal node.
If int, then consider min_samples_split as the minimum number.
If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum
number of samples for each split.</dd>
<dt><strong><code>max_leaf_nodes</code></strong> :&ensp;<code>int</code> or <code>None</code>, optional <code>(default=30)</code></dt>
<dd>Identify max_leaf_nodes amount of time series levels in the time series in best-first fashion. Best splits are
defined as relative reduction in impurity. If None then unlimited number of splits.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>boolean (Default=True)</code></dt>
<dd>A boolean to define whether to plot the time series and it's time series levels.</dd>
<dt><strong><code>equal_spaced</code></strong> :&ensp;<code>boolean (Default=True)</code></dt>
<dd>A boolean to define whether or not the time series is continuously measured or not. If False this will be taken
into account when plotting the X-axis of the plot.</dd>
<dt><strong><code>n_x_ticks</code></strong> :&ensp;<code>int (Default=10)</code></dt>
<dd>The amount of x-ticks you wish to show when plotting.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple (Default=(20,5))</code></dt>
<dd>The tuple used to specify the size of the plot if plot = True.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Demonstration of the function using time series data</p>
<pre><code class="language-python">&gt;&gt;&gt; ts_df = pd.read_csv(&quot;time_series_dataset.csv&quot;, index_col=0)
&gt;&gt;&gt; ts = ts_df[&quot;lorenz&quot;]
&gt;&gt;&gt; ts_levels_df, fig, ax = ts_levels(ts, ts_x=None, criterion=&quot;mse&quot;, max_depth=10, min_samples_leaf=1,
&gt;&gt;&gt;                          min_samples_split=2, max_leaf_nodes=30, plot=True, equal_spaced=True, n_x_ticks=10)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ts_levels(
    ts,
    ts_x=None,
    criterion=&#34;mse&#34;,
    max_depth=2,
    min_samples_leaf=1,
    min_samples_split=2,
    max_leaf_nodes=30,
    plot=True,
    equal_spaced=True,
    n_x_ticks=10,
    figsize=(20, 5),
):
    r&#34;&#34;&#34;Use recursive partitioning (DecisionTreeRegressor) to perform a &#39;classification&#39; of relatively stable levels in a timeseries.
    Parameters
    ---------
    ts: pandas DataFrame (column)
        A dataframe column containing a univariate time series from 1 person.
        Rows should indicate time, column should indicate the time series variable.
    ts_x: pandas DataFrame (column; Default=None)
        A dataframe column containing the corresponding timestamps to the aforementioned time series.
        If None is passed, the index of the time series will be used (Default = None).
    criterion: str (Default=&#34;mse&#34;)
        The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which
        is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each
        terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential
        splits, and “mae” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.
    max_depth: int or None, optional (default=2)
        The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves
        contain less than min_samples_split samples.
    min_samples_leaf: int, float, optional (default=1)
        The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered
        if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have
        the effect of smoothing the model, especially in regression.
        If int, then consider min_samples_leaf as the minimum number.
        If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are
        the minimum number of samples for each node.
    min_samples_split: int, float, optional (default=2)
        The minimum number of samples required to split an internal node.
        If int, then consider min_samples_split as the minimum number.
        If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum
        number of samples for each split.
    max_leaf_nodes: int or None, optional (default=30)
        Identify max_leaf_nodes amount of time series levels in the time series in best-first fashion. Best splits are
        defined as relative reduction in impurity. If None then unlimited number of splits.
    plot: boolean (Default=True)
        A boolean to define whether to plot the time series and it&#39;s time series levels.
    equal_spaced: boolean (Default=True)
        A boolean to define whether or not the time series is continuously measured or not. If False this will be taken
        into account when plotting the X-axis of the plot.
    n_x_ticks: int (Default=10)
        The amount of x-ticks you wish to show when plotting.
    figsize: tuple (Default=(20,5))
        The tuple used to specify the size of the plot if plot = True.

    Examples
    ---------
    Demonstration of the function using time series data
    &gt;&gt;&gt; ts_df = pd.read_csv(&#34;time_series_dataset.csv&#34;, index_col=0)
    &gt;&gt;&gt; ts = ts_df[&#34;lorenz&#34;]
    &gt;&gt;&gt; ts_levels_df, fig, ax = ts_levels(ts, ts_x=None, criterion=&#34;mse&#34;, max_depth=10, min_samples_leaf=1,
    &gt;&gt;&gt;                          min_samples_split=2, max_leaf_nodes=30, plot=True, equal_spaced=True, n_x_ticks=10)
    &#34;&#34;&#34;
    # Change ts to a numpy array
    if not isinstance(ts, np.ndarray):
        ts = ts.to_numpy()
    # Check whether ts has only one dimension
    if len(ts.shape) != 1:
        raise ValueError(&#34;ts is not one-dimensional&#34;)

    # Make sure the ts_x matches ts
    if ts_x is not None:
        # Change ts_x to a numpy array
        if not isinstance(ts_x, np.ndarray):
            ts_x = ts_x.to_numpy()
        # Check whether ts_x has only one dimension
        if len(ts.shape) != 1:
            raise ValueError(&#34;ts_x is not one-dimensional&#34;)
        # Check whether ts and tx_x have the same length
        if not len(ts) == len(ts_x):
            raise ValueError(&#34;ts and ts_x have different lengths&#34;)

    # predictor for the tree
    x = np.array(np.arange(len(ts)))
    x = x.reshape(-1, 1)

    dtr = DecisionTreeRegressor(
        criterion=criterion,
        max_depth=max_depth,
        min_samples_leaf=min_samples_leaf,
        min_samples_split=min_samples_split,
        max_leaf_nodes=max_leaf_nodes,
    )
    tree = dtr.fit(x, ts)
    p = tree.predict(x)

    fig = None
    ax = None

    if plot:
        # Select n indices
        idx = np.round(np.linspace(0, len(x) - 1, n_x_ticks)).astype(int)

        # X ticks and labels based on indices
        if ts_x is None:
            xticks = flatten(x[idx].tolist())
            xlabs = flatten(x[idx].tolist())
            x_plot = x

        # X labels based on ts_x
        else:
            # Plot using x indices and ts_x labels
            if equal_spaced:
                x_plot = x
                xticks = flatten(x[idx].tolist())
                xlabs = flatten(ts_x[idx].tolist())

            # Plot using ts_x indices and ts_x labels
            else:
                x_plot = ts_x
                xticks = flatten(ts_x[idx].tolist())
                xlabs = flatten(ts_x[idx].tolist())

        # _ = plt.figure(figsize=(20, 7))
        fig, ax = plt.subplots(figsize=figsize)
        _ = plt.scatter(
            x_plot,
            ts,
            s=20,
            edgecolor=&#34;#2167C5&#34;,
            c=&#34;#EB5E23&#34;,
            label=&#34;Original Time Series&#34;,
        )
        _ = plt.plot(
            x_plot,
            p,
            c=&#34;white&#34;,
            path_effects=[pe.Stroke(linewidth=5, foreground=&#34;#2167C5&#34;), pe.Normal()],
            label=&#34;Time Series Levels&#34;,
        )
        _ = plt.xticks(xticks, xlabs)
        _ = plt.ylabel(&#34;Amount&#34;)
        _ = plt.xlabel(&#34;Time&#34;)
        _ = plt.legend()
        _ = plt.title(&#34;Time Series Levels Plot&#34;)
        # _ = plt.show()

    # Store t_steps, original ts and ts_levels to a dataframe
    df_result = pd.DataFrame(
        {
            &#34;t_steps&#34;: flatten(x.tolist()),
            &#34;original_ts&#34;: ts.tolist(),
            &#34;ts_levels&#34;: p.tolist(),
        }
    )

    # Add the additional ts_x
    if ts_x is not None:
        df_result[&#34;ts_x&#34;] = ts_x

    return df_result, fig, ax</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jmspack" href="index.html">jmspack</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="jmspack.NLTSA.complexity_resonance" href="#jmspack.NLTSA.complexity_resonance">complexity_resonance</a></code></li>
<li><code><a title="jmspack.NLTSA.complexity_resonance_diagram" href="#jmspack.NLTSA.complexity_resonance_diagram">complexity_resonance_diagram</a></code></li>
<li><code><a title="jmspack.NLTSA.cumulative_complexity_peaks" href="#jmspack.NLTSA.cumulative_complexity_peaks">cumulative_complexity_peaks</a></code></li>
<li><code><a title="jmspack.NLTSA.cumulative_complexity_peaks_plot" href="#jmspack.NLTSA.cumulative_complexity_peaks_plot">cumulative_complexity_peaks_plot</a></code></li>
<li><code><a title="jmspack.NLTSA.distribution_uniformity" href="#jmspack.NLTSA.distribution_uniformity">distribution_uniformity</a></code></li>
<li><code><a title="jmspack.NLTSA.flatten" href="#jmspack.NLTSA.flatten">flatten</a></code></li>
<li><code><a title="jmspack.NLTSA.fluctuation_intensity" href="#jmspack.NLTSA.fluctuation_intensity">fluctuation_intensity</a></code></li>
<li><code><a title="jmspack.NLTSA.ts_levels" href="#jmspack.NLTSA.ts_levels">ts_levels</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
